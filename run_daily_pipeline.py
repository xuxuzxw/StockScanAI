# StockScanAI/run_daily_pipeline.py
#
# ã€V2.2 æ ¸å¿ƒä¼˜åŒ–ã€‘
# ç›®çš„ï¼šå°†æ•°æ®æŠ½å–å’Œå› å­è®¡ç®—ä¸¤ä¸ªç‹¬ç«‹çš„åå°ä»»åŠ¡ï¼Œåˆå¹¶ä¸ºä¸€ä¸ªç»Ÿä¸€ã€å¥å£®çš„æ•°æ®ç®¡é“ã€‚
#      æ¶ˆé™¤äº†å¯¹æœ¬åœ°HDF5ç¼“å­˜æ–‡ä»¶çš„ä¾èµ–ï¼Œå®ç°äº†æ•°æ®åœ¨å†…å­˜ä¸­çš„æ— ç¼æµè½¬ã€‚

import os
import time
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
from sqlalchemy import text

# å¯¼å…¥é¡¹ç›®æ¨¡å—
import data
import quant_engine
from logger_config import log
from progress_tracker import ProgressTracker

# ã€V2.2 é‡æ„ã€‘å°†å› å­åˆ—è¡¨çš„å®šä¹‰ç§»å…¥æ­¤ç»Ÿä¸€ç®¡é“è„šæœ¬ä¸­ï¼Œç§»é™¤å¯¹æ—§æ–‡ä»¶çš„ä¾èµ–
FACTORS_TO_CALCULATE = [
    "pe_ttm",
    "roe",
    "growth_revenue_yoy",
    "debt_to_assets",
    "momentum",
    "volatility",
    "net_inflow_ratio",
    "holder_num_change_ratio",
    "major_shareholder_net_buy_ratio",
    "top_list_net_buy_amount",
    "dividend_yield",
    "forecast_growth_rate",
    "repurchase_ratio",
    "block_trade_ratio",
]


def extract_data(trade_date: str) -> dict:
    """
    æ­¥éª¤ä¸€ï¼šæ‰§è¡Œæ‰€æœ‰è€—æ—¶çš„æ•°æ®æŠ½å–å’Œé¢„å¤„ç†å·¥ä½œã€‚
    ã€V2.7 å¢å¼ºç‰ˆ - å¢åŠ è¿›åº¦æ˜¾ç¤ºã€‘
    """
    log.info("=" * 60)
    log.info(f"ğŸ“Š å¼€å§‹ä¸º {trade_date} æŠ½å–å…¨å¸‚åœºåŸå§‹æ•°æ®")
    log.info("=" * 60)

    dm = data.DataManager()
    stock_list = dm.get_stock_basic()
    ts_codes = stock_list["ts_code"].tolist()
    
    log.info(f"ğŸ¯ ç›®æ ‡è‚¡ç¥¨æ•°é‡: {len(ts_codes)} åª")

    # --- 1. æ‰¹é‡è·å–æˆªé¢æ•°æ® ---
    log.info("ğŸ“ˆ è·å–å½“æ—¥æˆªé¢æ•°æ®...")
    
    data_sources = [
        ("åŸºæœ¬æŒ‡æ ‡", lambda: dm.pro.daily_basic(trade_date=trade_date)),
        ("èµ„é‡‘æµå‘", lambda: dm.pro.moneyflow(trade_date=trade_date)),
        ("é¾™è™æ¦œ", lambda: dm.pro.top_list(trade_date=trade_date)),
        ("å¤§å®—äº¤æ˜“", lambda: dm.pro.block_trade(trade_date=trade_date))
    ]
    
    results = {}
    for name, func in data_sources:
        try:
            log.info(f"  ğŸ“Š è·å–{name}æ•°æ®...")
            data = func()
            results[name] = data
            count = len(data) if data is not None and not data.empty else 0
            log.info(f"  âœ… {name}: {count} æ¡è®°å½•")
        except Exception as e:
            log.warning(f"  âš ï¸  {name}è·å–å¤±è´¥: {e}")
            results[name] = pd.DataFrame()
    
    daily_basics_df = results.get("åŸºæœ¬æŒ‡æ ‡", pd.DataFrame())
    money_flow_df = results.get("èµ„é‡‘æµå‘", pd.DataFrame())
    top_list_df = results.get("é¾™è™æ¦œ", pd.DataFrame())
    block_trade_df = results.get("å¤§å®—äº¤æ˜“", pd.DataFrame())

    # --- 2. ã€ç¼“å­˜ä¼˜å…ˆã€‘è·å–æ—¶åºä»·æ ¼æ•°æ® ---
    log.info("  å¼€å§‹è·å–å„è‚¡ç¥¨çš„å†å²ä»·æ ¼ (ç¼“å­˜ä¼˜å…ˆ)...")
    start_date_lookback = (pd.to_datetime(trade_date) - timedelta(days=90)).strftime("%Y%m%d")
    prices_dict = {}

# æ­¥éª¤ A: æ£€æŸ¥æ•°æ®åº“ç¼“å­˜
    try:
        min_trading_days = 55
        # V3.1 ç»ˆæå¥å£®æ€§ä¿®å¤ï¼šåœ¨SQLæŸ¥è¯¢ä¸­æ˜ç¡®è¿›è¡Œæ—¥æœŸç±»å‹è½¬æ¢ï¼Œå½»åº•è§£å†³ç¼“å­˜æ£€æŸ¥å¤±æ•ˆé—®é¢˜
        query = text("""
            SELECT ts_code FROM ts_daily
            WHERE trade_date BETWEEN TO_DATE(:start_date, 'YYYYMMDD') AND TO_DATE(:end_date, 'YYYYMMDD') 
              AND ts_code = ANY(:ts_codes)
            GROUP BY ts_code 
            HAVING COUNT(trade_date) >= :min_days
        """)
        with dm.engine.connect() as conn:
            cached_stocks_result = conn.execute(query, {
                "start_date": start_date_lookback, "end_date": trade_date,
                "ts_codes": ts_codes, "min_days": min_trading_days
            }).fetchall()
        cached_stocks = {row[0] for row in cached_stocks_result}
        log.info(f"    æ•°æ®åº“ç¼“å­˜æ£€æŸ¥å®Œæˆï¼š{len(cached_stocks)}/{len(ts_codes)} åªè‚¡ç¥¨å·²æœ‰å®Œæ•´æœ¬åœ°æ•°æ®ã€‚")
    except Exception as e:
        log.warning(f"    æ£€æŸ¥ç¼“å­˜å¤±è´¥: {e}ã€‚å°†å°è¯•å…¨é‡ä¸‹è½½ã€‚")
        cached_stocks = set()

    # æ­¥éª¤ B: ä»…ä¸‹è½½æ— ç¼“å­˜çš„æ•°æ® (å¸¦é‡è¯•æœºåˆ¶)
    stocks_to_download = sorted(list(set(ts_codes) - cached_stocks))
    if stocks_to_download:
        log.info(f"    éœ€è¦ä»ç½‘ç»œä¸‹è½½ {len(stocks_to_download)} åªè‚¡ç¥¨çš„æ•°æ®...")
        
        # --- ã€æ–°å¢ã€‘å®Œæ•´æ€§æ ¡éªŒä¸é‡è¯•é€»è¾‘ ---
        downloaded_data_raw = {}
        max_retries = 3  # V2.9 æå‡å¥å£®æ€§ï¼šå¢åŠ ä¸Šå±‚é‡è¯•æ¬¡æ•°
        for attempt in range(max_retries + 1):
            # V2.9 ä¿®æ­£ï¼šæ¯æ¬¡é‡è¯•æ—¶ï¼Œéƒ½éœ€è¦é‡æ–°è®¡ç®—è¿˜éœ€è¦ä¸‹è½½çš„åˆ—è¡¨
            needed = [
                code
                for code in stocks_to_download
                if code not in downloaded_data_raw
                or downloaded_data_raw[code] is None
                or downloaded_data_raw[code].empty
            ]
            if not needed:
                break
            
            if attempt > 0:
                log.warning(f"    ç¬¬ {attempt} æ¬¡é‡è¯•ï¼Œä¸‹è½½å‰©ä½™çš„ {len(needed)} åªè‚¡ç¥¨...")

            chunk_size = 150
            for i in range(0, len(needed), chunk_size):
                chunk = needed[i : i + chunk_size]
                log.info(f"      æ­£åœ¨ä¸‹è½½å— {i//chunk_size + 1}/{len(needed)//chunk_size + 1} (è‚¡ç¥¨ {i+1}-{i+len(chunk)})...")
                chunk_results = dm.run_batch_download(chunk, start_date_lookback, trade_date)
                downloaded_data_raw.update(chunk_results)
            
        # æ ¡éªŒæœ€ç»ˆç»“æœ
        missing_stocks = [code for code in stocks_to_download if code not in downloaded_data_raw or downloaded_data_raw[code] is None or downloaded_data_raw[code].empty]
        if missing_stocks:
            log.error(f"    è­¦å‘Šï¼šç»è¿‡ {max_retries} æ¬¡é‡è¯•åï¼Œä»æœ‰ {len(missing_stocks)} åªè‚¡ç¥¨æ•°æ®ä¸‹è½½å¤±è´¥ã€‚")
        # --- æ ¡éªŒé€»è¾‘ç»“æŸ ---

        for code, df in downloaded_data_raw.items():
            if df is not None and not df.empty:
                df['trade_date'] = pd.to_datetime(df['trade_date'])
                prices_dict[code] = df.set_index("trade_date")["close"]

    # æ­¥éª¤ C: ä»æ•°æ®åº“åŠ è½½å·²ç¼“å­˜çš„æ•°æ®
    if cached_stocks:
        log.info(f"    æ­£åœ¨ä»æ•°æ®åº“åˆ†æ‰¹åŠ è½½ {len(cached_stocks)} åªå·²ç¼“å­˜è‚¡ç¥¨çš„æ•°æ®...")
        # (æ­¤å¤„é€»è¾‘ä¸å˜)
        cached_list = list(cached_stocks)
        chunk_size = 500
        for i in range(0, len(cached_list), chunk_size):
            chunk = cached_list[i:i+chunk_size]
            query = text("""
                SELECT ts_code, trade_date, close FROM ts_daily
                WHERE trade_date BETWEEN :start_date AND :end_date AND ts_code = ANY(:ts_codes)
            """)
            with dm.engine.connect() as conn:
                cached_df = pd.read_sql(query, conn, params={
                    "start_date": start_date_lookback, "end_date": trade_date, "ts_codes": chunk
                })
            
            if not cached_df.empty:
                cached_df['trade_date'] = pd.to_datetime(cached_df['trade_date'])
                for code, group in cached_df.groupby('ts_code'):
                    prices_dict[code] = group.set_index('trade_date')['close']

    # --- 3. åˆå¹¶ä¸æ ¼å¼åŒ– ---
    daily_prices_df = pd.DataFrame(prices_dict)

    log.info("ã€æ•°æ®æŠ½å–ã€‘æ‰€æœ‰åŸå§‹æ•°æ®æå–å®Œæ¯•ã€‚")

    return {
        "stock_list": stock_list,
        "ts_codes": ts_codes,
        "daily_prices_df": daily_prices_df,
        "daily_basics_df": daily_basics_df,
        "money_flow_df": money_flow_df,
        "top_list_df": top_list_df,
        "block_trade_df": block_trade_df,
    }


def calculate_and_save_factors(trade_date: str, raw_data: dict, all_fina_data: pd.DataFrame):
    """
    æ­¥éª¤äºŒï¼šæ‰§è¡Œå› å­è®¡ç®—ä¸å­˜å‚¨ã€‚
    åŸ factor_calculator.py çš„æ ¸å¿ƒé€»è¾‘ã€‚
    :param trade_date: äº¤æ˜“æ—¥æœŸ YYYYMMDD
    :param raw_data: ä» extract_data å‡½æ•°è·å–çš„åŸå§‹æ•°æ®å­—å…¸ã€‚
    :param all_fina_data: ã€æ–°å¢ã€‘é¢„å…ˆè·å–çš„å…¨å¸‚åœºè´¢åŠ¡æ•°æ®ã€‚
    """
    log.info("ã€å› å­è®¡ç®—ã€‘å¼€å§‹æ··åˆæ¨¡å¼è®¡ç®—æ‰€æœ‰å› å­...")
    dm = data.DataManager()
    ff = quant_engine.FactorFactory(_data_manager=dm)
    trade_date_dt = pd.to_datetime(trade_date)

    # ä»ä¼ å…¥çš„å­—å…¸ä¸­è§£åŒ…æ•°æ®
    ts_codes = raw_data["ts_codes"]
    daily_prices_df = raw_data["daily_prices_df"]
    daily_basics_df = raw_data["daily_basics_df"]
    money_flow_df = raw_data["money_flow_df"]

    results = {}

    # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šé«˜æ€§èƒ½çš„å‘é‡åŒ–è®¡ç®— ---
    log.info("  æ­£åœ¨å‘é‡åŒ–è®¡ç®—ï¼šä»·æ ¼ç±»ä¸æˆªé¢ç±»å› å­...")
    vectorized_factors = ["pe_ttm", "momentum", "volatility", "net_inflow_ratio"]
    financial_factors = ["roe", "growth_revenue_yoy", "debt_to_assets"] # å®šä¹‰è´¢åŠ¡å› å­

    if daily_prices_df is not None and not daily_prices_df.empty:
        prices = daily_prices_df.ffill()
        if len(prices) >= 21:
            results["momentum"] = prices.iloc[-1] / prices.iloc[-21] - 1
        if len(prices) >= 20:
            results["volatility"] = np.log(prices / prices.shift(1)).iloc[
                -20:
            ].std() * np.sqrt(252)

    if daily_basics_df is not None and not daily_basics_df.empty:
        basics = daily_basics_df.set_index("ts_code")
        results["pe_ttm"] = basics["pe_ttm"]
        if money_flow_df is not None and not money_flow_df.empty:
            flow = money_flow_df.drop(columns=["trade_date"]).set_index("ts_code")
            combined = basics.join(flow, how="inner")
            if not combined.empty and "amount" in combined.columns:
                amount_yuan = combined["amount"] * 1000
                net_inflow_yuan = (
                    combined["buy_lg_amount"] - combined["sell_lg_amount"]
                ) * 10000
                results["net_inflow_ratio"] = net_inflow_yuan.divide(
                    amount_yuan
                ).fillna(0)

    # --- ã€æ€§èƒ½ä¼˜åŒ–ã€‘ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºäºé¢„å–æ•°æ®çš„è´¢åŠ¡å› å­è®¡ç®— ---
    log.info("  æ­£åœ¨åŸºäºé¢„å–æ•°æ®è®¡ç®—ï¼šè´¢åŠ¡ç±»å› å­...")
    if all_fina_data is not None and not all_fina_data.empty:
        # å¯¹é¢„å–çš„æ‰€æœ‰è´¢åŠ¡æ•°æ®è¿›è¡Œä¸€æ¬¡PITç­›é€‰
        pit_fina_data = all_fina_data.groupby('ts_code', include_groups=False).apply(
            lambda x: dm.get_pit_financial_data(x, trade_date)
        ).reset_index(drop=True)
        
        pit_fina_data = pit_fina_data.set_index('ts_code')
        results["roe"] = pit_fina_data["roe"]
        results["growth_revenue_yoy"] = pit_fina_data["or_yoy"] # Tushareä¸­å­—æ®µä¸º or_yoy
        results["debt_to_assets"] = pit_fina_data["debt_to_assets"]


    # --- ç¬¬ä¸‰éƒ¨åˆ†ï¼šå‰©ä½™å› å­ç»Ÿä¸€å¾ªç¯è®¡ç®— ---
    log.info("  æ­£åœ¨å¾ªç¯è®¡ç®—ï¼šç­¹ç ä¸ä»·å€¼ç±»å› å­...")
    params = {
        "date": trade_date,
        "start_date": (trade_date_dt - timedelta(days=90)).strftime("%Y%m%d"),
        "end_date": trade_date,
        "top_list_df": raw_data["top_list_df"],
        "block_trade_df": raw_data["block_trade_df"],
    }
    loop_factors = [f for f in FACTORS_TO_CALCULATE if f not in vectorized_factors and f not in financial_factors]

    for factor in loop_factors:
        log.info(f"    æ­£åœ¨è®¡ç®—å› å­: {factor}...")
        factor_series = pd.Series(index=ts_codes, dtype=float)
        for code in ts_codes:
            params["ts_code"] = code
            factor_series[code] = ff.calculate(factor, **params)
        results[factor] = factor_series

    # --- ç¬¬å››éƒ¨åˆ†ï¼šæ•´åˆä¸å­˜å‚¨ ---
    log.info("ã€æ•°æ®å…¥åº“ã€‘å¼€å§‹å­˜å‚¨å› å­æ•°æ®...")
    final_df = pd.DataFrame(results).reset_index().rename(columns={"index": "ts_code"})
    long_df = final_df.melt(
        id_vars="ts_code", var_name="factor_name", value_name="factor_value"
    ).dropna()
    long_df["trade_date"] = pd.to_datetime(trade_date)

    if long_df.empty:
        log.warning("æ²¡æœ‰æœ‰æ•ˆçš„å› å­æ•°æ®å¯ä»¥å­˜å…¥æ•°æ®åº“ã€‚")
        return

    try:
        with dm.engine.connect() as connection:
            with connection.begin():
                delete_sql = text(
                    "DELETE FROM factors_exposure WHERE trade_date = :trade_date"
                )
                connection.execute(delete_sql, {"trade_date": trade_date})
                long_df.to_sql(
                    "factors_exposure",
                    connection,
                    if_exists="append",
                    index=False,
                    chunksize=10000,
                )
        log.info(f"æˆåŠŸä¸º {trade_date} å†™å…¥ {len(long_df)} æ¡å› å­æ•°æ®ã€‚")
    except Exception as e:
        log.critical(f"å› å­æ•°æ®å†™å…¥æ•°æ®åº“æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)


def run_daily_pipeline():
    """
    æ‰§è¡Œæ¯æ—¥æ•°æ®ç®¡é“çš„ä¸»å·¥ä½œæµã€‚
    - å…·å¤‡æ™ºèƒ½è¡¥æ¼åŠŸèƒ½ï¼Œè‡ªåŠ¨è¡¥é½å†å²ç¼ºå¤±æ•°æ®ã€‚
    """
    dm = data.DataManager()

    # 1. ç¡®å®šéœ€è¦å¤„ç†çš„äº¤æ˜“æ—¥åˆ—è¡¨
    log.info("===== æ­£åœ¨ç¡®å®šéœ€è¦å¤„ç†çš„äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆåŒ…æ‹¬å†å²è¡¥æ¼ï¼‰ =====")
    try:
        # è·å–æ•°æ®åº“ä¸­factors_exposureè¡¨çš„æœ€æ–°æ—¥æœŸ
        with dm.engine.connect() as connection:
            query_max_date = text("SELECT MAX(trade_date) FROM factors_exposure")
            last_processed_date_db = connection.execute(query_max_date).scalar_one_or_none()

        # ç¡®å®šå¼€å§‹è·å–äº¤æ˜“æ—¥å†çš„æ—¥æœŸ
        if last_processed_date_db:
            # ä»æ•°æ®åº“æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹æ£€æŸ¥
            start_cal_date = (pd.to_datetime(last_processed_date_db) + timedelta(days=1)).strftime("%Y%m%d")
        else:
            # å¦‚æœæ•°æ®åº“ä¸ºç©ºï¼Œåˆ™ä»ä¸€ä¸ªå¾ˆæ—©çš„æ—¥æœŸå¼€å§‹ï¼ˆä¾‹å¦‚2010å¹´ï¼‰
            start_cal_date = "20100101"
        
        end_cal_date = datetime.now().strftime("%Y%m%d")

        # è·å–Tushareäº¤æ˜“æ—¥å†
        cal_df = dm.pro.trade_cal(
            exchange="",
            start_date=start_cal_date,
            end_date=end_cal_date,
        )
        all_trade_dates_ts = set(cal_df[cal_df["is_open"] == 1]["cal_date"].tolist())

        # è·å–æ•°æ®åº“ä¸­å·²æœ‰çš„factors_exposureæ—¥æœŸ
        existing_factors_dates = set()
        if last_processed_date_db:
            with dm.engine.connect() as connection:
                query_existing_dates = text(
                    "SELECT DISTINCT trade_date FROM factors_exposure WHERE trade_date >= :start_date AND trade_date <= :end_date"
                )
                result = connection.execute(query_existing_dates, {
                    "start_date": start_cal_date, "end_date": end_cal_date
                }).fetchall()
                existing_factors_dates = {row[0] for row in result}

        # æ‰¾å‡ºæ‰€æœ‰ç¼ºå¤±çš„äº¤æ˜“æ—¥
        dates_to_process = sorted(list(all_trade_dates_ts - existing_factors_dates))

        if not dates_to_process:
            log.info("æ‰€æœ‰å†å²æ•°æ®å‡å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€è¡¥æ¼ã€‚ä»»åŠ¡ç»“æŸã€‚")
            return
        else:
            log.info(f"å‘ç° {len(dates_to_process)} ä¸ªç¼ºå¤±æˆ–å¾…å¤„ç†çš„äº¤æ˜“æ—¥ã€‚")

    except Exception as e:
        log.error(f"ç¡®å®šäº¤æ˜“æ—¥åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return

    # 2. å¾ªç¯å¤„ç†æ¯ä¸ªäº¤æ˜“æ—¥
    for current_trade_date in dates_to_process:
        log.info(f"===== å¼€å§‹æ‰§è¡Œ {current_trade_date} çš„ç»Ÿä¸€æ•°æ®ç®¡é“ä»»åŠ¡ =====")
        start_time = time.time()

        # --- å‰ç½®æ£€æŸ¥æœºåˆ¶ (é’ˆå¯¹å½“å‰æ—¥æœŸ) ---
        try:
            with dm.engine.connect() as connection:
                check_sql = text(
                    "SELECT 1 FROM factors_exposure WHERE trade_date = :trade_date LIMIT 1"
                )
                result = connection.execute(
                    check_sql, {"trade_date": current_trade_date}
                ).scalar_one_or_none()
            if result == 1:
                log.info(
                    f"æ£€æµ‹åˆ°æ•°æ®åº“ä¸­å·²å­˜åœ¨ {current_trade_date} çš„å› å­æ•°æ®ã€‚è·³è¿‡ã€‚"
                )
                continue # è·³è¿‡å½“å‰æ—¥æœŸï¼Œå¤„ç†ä¸‹ä¸€ä¸ª
        except Exception as e:
            log.error(f"åœ¨æ‰§è¡Œå‰ç½®æ£€æŸ¥æ—¶å‘ç”Ÿæ•°æ®åº“é”™è¯¯: {e}", exc_info=True)
            continue # å‘ç”Ÿé”™è¯¯åˆ™è·³è¿‡å½“å‰æ—¥æœŸ

        # --- æ‰§è¡Œæ•°æ®ç®¡é“ ---
        # æ­¥éª¤ä¸€ï¼šæŠ½å–æ•°æ®
        raw_data_dict = extract_data(current_trade_date)

        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘æ­¥éª¤äºŒï¼šæ‰¹é‡é¢„å–å…¨å¸‚åœºè´¢åŠ¡æ•°æ®
        log.info("ã€æ€§èƒ½ä¼˜åŒ–ã€‘å¼€å§‹æ‰¹é‡é¢„å–å…¨å¸‚åœºè´¢åŠ¡æ•°æ®...")
        try:
            all_fina_data_list = []
            ts_codes_for_fina = raw_data_dict["ts_codes"]
            log.info(f"  æ­£åœ¨ä¸º {len(ts_codes_for_fina)} åªè‚¡ç¥¨æ‰¹é‡é¢„å–è´¢åŠ¡æ•°æ®...")
            
            # ä¼˜åŒ–ï¼šåªå¯¹éœ€è¦æ›´æ–°çš„è‚¡ç¥¨è·å–è´¢åŠ¡æ•°æ®
            # è´¢åŠ¡æ•°æ®å·²åœ¨data.pyä¸­å®ç°æ™ºèƒ½ç¼“å­˜ï¼Œæ­¤å¤„æ— éœ€é¢å¤–åˆ¤æ–­
            for i, code in enumerate(ts_codes_for_fina):
                if i % 100 == 0: 
                    log.info(f"    å·²å¤„ç† {i}/{len(ts_codes_for_fina)} åªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®...")
                try:
                    df_fina = dm.get_fina_indicator(ts_code=code, force_update=False) # ä¸å¼ºåˆ¶æ›´æ–°ï¼Œåˆ©ç”¨å†…éƒ¨ç¼“å­˜
                    if df_fina is not None and not df_fina.empty:
                        all_fina_data_list.append(df_fina)
                except Exception as e:
                    log.warning(f"    ä¸ºè‚¡ç¥¨ {code} é¢„å–è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
                    continue
            
            if all_fina_data_list:
                all_fina_data = pd.concat(all_fina_data_list).drop_duplicates(subset=['ts_code', 'end_date'])
                log.info(f"æ‰¹é‡é¢„å–å®Œæˆï¼Œå…±è·å– {len(all_fina_data)} æ¡è´¢åŠ¡è®°å½•ã€‚")
            else:
                all_fina_data = pd.DataFrame()
                log.warning("æœªèƒ½æˆåŠŸé¢„å–ä»»ä½•è´¢åŠ¡æ•°æ®ã€‚")
        except Exception as e:
            log.error(f"æ‰¹é‡é¢„å–è´¢åŠ¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
            all_fina_data = pd.DataFrame()

        # æ­¥éª¤ä¸‰ï¼šè®¡ç®—å¹¶å­˜å‚¨å› å­
        calculate_and_save_factors(current_trade_date, raw_data_dict, all_fina_data)

        duration = time.time() - start_time
        log.info(
            f"===== {current_trade_date} çš„ç»Ÿä¸€æ•°æ®ç®¡é“ä»»åŠ¡å®Œæˆï¼æ€»è€—æ—¶: {duration:.2f} ç§’ã€‚====="
        )

    log.info("===== æ‰€æœ‰å¾…å¤„ç†äº¤æ˜“æ—¥çš„ç»Ÿä¸€æ•°æ®ç®¡é“ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼ =====")


if __name__ == "__main__":
    run_daily_pipeline()
    # input("\nä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼ŒæŒ‰ Enter é”®é€€å‡º...") # ç§»é™¤inputï¼Œé¿å…éäº¤äº’å¼è¿è¡ŒæŒ‚èµ·
