# StockScanAI/run_daily_pipeline.py
#
# ã€V2.2 æ ¸å¿ƒä¼˜åŒ–ã€‘
# ç›®çš„ï¼šå°†æ•°æ®æŠ½å–å’Œå› å­è®¡ç®—ä¸¤ä¸ªç‹¬ç«‹çš„åå°ä»»åŠ¡ï¼Œåˆå¹¶ä¸ºä¸€ä¸ªç»Ÿä¸€ã€å¥å£®çš„æ•°æ®ç®¡é“ã€‚
#      æ¶ˆé™¤äº†å¯¹æœ¬åœ°HDF5ç¼“å­˜æ–‡ä»¶çš„ä¾èµ–ï¼Œå®ç°äº†æ•°æ®åœ¨å†…å­˜ä¸­çš„æ— ç¼æµè½¬ã€‚

import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field

import numpy as np
import pandas as pd
from sqlalchemy import text

# å¯¼å…¥é¡¹ç›®æ¨¡å—
import data
import quant_engine
from logger_config import log
from progress_tracker import ProgressTracker
from system_check_error_handler import safe_execute, retry_on_failure, CheckResult

# ã€V2.2 é‡æ„ã€‘å°†å› å­åˆ—è¡¨çš„å®šä¹‰ç§»å…¥æ­¤ç»Ÿä¸€ç®¡é“è„šæœ¬ä¸­ï¼Œç§»é™¤å¯¹æ—§æ–‡ä»¶çš„ä¾èµ–
@dataclass
class PipelineConfig:
    """æ•°æ®ç®¡é“é…ç½®å¸¸é‡ - ä½¿ç”¨dataclassæé«˜ç±»å‹å®‰å…¨æ€§"""
    
    # å› å­åˆ—è¡¨
    FACTORS_TO_CALCULATE: List[str] = field(default_factory=lambda: [
        "pe_ttm", "roe", "growth_revenue_yoy", "debt_to_assets",
        "momentum", "volatility", "net_inflow_ratio",
        "holder_num_change_ratio", "major_shareholder_net_buy_ratio",
        "top_list_net_buy_amount", "dividend_yield", "forecast_growth_rate",
        "repurchase_ratio", "block_trade_ratio"
    ])
    
    # æ—¶é—´é…ç½®
    MARKET_CLOSE_TIME: str = "15:30"
    LOOKBACK_DAYS: int = 90
    CALENDAR_BUFFER_DAYS: int = 30
    CALENDAR_FUTURE_DAYS: int = 5
    
    # æ•°æ®è´¨é‡é˜ˆå€¼
    MIN_TRADING_DAYS: int = 55
    MIN_FACTOR_COUNT_THRESHOLD: int = 100
    
    # æ‰¹å¤„ç†é…ç½®
    DOWNLOAD_CHUNK_SIZE: int = 150
    DB_CHUNK_SIZE: int = 500
    DB_WRITE_CHUNK_SIZE: int = 10000
    MAX_RETRIES: int = 3
    
    # å†å²æ•°æ®èµ·å§‹æ—¥æœŸ
    HISTORICAL_START_DATE: str = "20100101"
    
    # å› å­è®¡ç®—å¸¸é‡
    MOMENTUM_LOOKBACK_DAYS: int = 21
    VOLATILITY_LOOKBACK_DAYS: int = 20
    TRADING_DAYS_PER_YEAR: int = 252
    
    @classmethod
    def create_for_environment(cls, env: str = "production") -> 'PipelineConfig':
        """Create configuration for specific environment"""
        if env == "development":
            return cls(
                DOWNLOAD_CHUNK_SIZE=50,  # Smaller chunks for dev
                DB_CHUNK_SIZE=100,
                MAX_RETRIES=1,
                MIN_TRADING_DAYS=20  # Less strict for dev
            )
        elif env == "testing":
            return cls(
                FACTORS_TO_CALCULATE=["pe_ttm", "momentum"],  # Only basic factors
                DOWNLOAD_CHUNK_SIZE=10,
                DB_CHUNK_SIZE=50,
                MAX_RETRIES=1,
                LOOKBACK_DAYS=30
            )
        else:  # production
            return cls()

def _batch_fetch_financial_data(dm, ts_codes: List[str]) -> pd.DataFrame:
    """
    ä¼˜åŒ–çš„æ‰¹é‡è´¢åŠ¡æ•°æ®è·å–å‡½æ•°
    ä½¿ç”¨å¹¶å‘å¤„ç†å’Œæ™ºèƒ½ç¼“å­˜
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from progress_tracker import ProgressTracker
    
    log.info("ã€æ€§èƒ½ä¼˜åŒ–ã€‘å¼€å§‹æ‰¹é‡é¢„å–å…¨å¸‚åœºè´¢åŠ¡æ•°æ®...")
    
    try:
        all_fina_data_list = []
        failed_codes = []
        
        # ä½¿ç”¨è¿›åº¦è·Ÿè¸ªå™¨
        tracker = ProgressTracker(len(ts_codes), "è´¢åŠ¡æ•°æ®é¢„å–")
        
        def fetch_single_stock_financial_data(code: str) -> Optional[pd.DataFrame]:
            """è·å–å•åªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®"""
            try:
                df_fina = dm.get_fina_indicator(ts_code=code, force_update=False)
                return df_fina if df_fina is not None and not df_fina.empty else None
            except Exception as e:
                log.debug(f"è‚¡ç¥¨ {code} è´¢åŠ¡æ•°æ®è·å–å¤±è´¥: {e}")
                return None
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†ï¼ˆI/Oå¯†é›†å‹ä»»åŠ¡ï¼‰
        max_workers = min(4, len(ts_codes) // 100 + 1)  # åŠ¨æ€è°ƒæ•´çº¿ç¨‹æ•°
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_code = {
                executor.submit(fetch_single_stock_financial_data, code): code 
                for code in ts_codes
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_code):
                code = future_to_code[future]
                try:
                    result = future.result()
                    if result is not None:
                        all_fina_data_list.append(result)
                        tracker.update(1, success=True)
                    else:
                        failed_codes.append(code)
                        tracker.update(1, success=False)
                except Exception as e:
                    log.debug(f"å¤„ç†è‚¡ç¥¨ {code} ç»“æœæ—¶å‡ºé”™: {e}")
                    failed_codes.append(code)
                    tracker.update(1, success=False)
        
        # å®Œæˆè¿›åº¦è·Ÿè¸ª
        summary = tracker.finish()
        
        # åˆå¹¶æ•°æ®
        if all_fina_data_list:
            all_fina_data = pd.concat(all_fina_data_list, ignore_index=True)
            # å»é‡å¹¶æ’åº
            all_fina_data = all_fina_data.drop_duplicates(subset=['ts_code', 'end_date']).sort_values(['ts_code', 'end_date'])
            log.info(f"âœ… æ‰¹é‡é¢„å–å®Œæˆ: {len(all_fina_data)} æ¡è´¢åŠ¡è®°å½•ï¼ŒæˆåŠŸç‡: {summary['success_rate']:.1%}")
        else:
            all_fina_data = pd.DataFrame()
            log.warning("âš ï¸ æœªèƒ½æˆåŠŸé¢„å–ä»»ä½•è´¢åŠ¡æ•°æ®")
        
        if failed_codes:
            log.warning(f"âš ï¸ {len(failed_codes)} åªè‚¡ç¥¨è´¢åŠ¡æ•°æ®è·å–å¤±è´¥")
        
        return all_fina_data
        
    except Exception as e:
        log.error(f"âŒ æ‰¹é‡é¢„å–è´¢åŠ¡æ•°æ®å¤±è´¥: {e}", exc_info=True)
        return pd.DataFrame()


FACTORS_TO_CALCULATE = PipelineConfig().FACTORS_TO_CALCULATE


def extract_data(trade_date: str, config: Optional[PipelineConfig] = None) -> dict:
    """
    æ­¥éª¤ä¸€ï¼šæ‰§è¡Œæ‰€æœ‰è€—æ—¶çš„æ•°æ®æŠ½å–å’Œé¢„å¤„ç†å·¥ä½œã€‚
    ã€V2.7 å¢å¼ºç‰ˆ - å¢åŠ è¿›åº¦æ˜¾ç¤ºã€‘
    
    Args:
        trade_date: äº¤æ˜“æ—¥æœŸ (YYYYMMDDæ ¼å¼)
        config: ç®¡é“é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        
    Returns:
        dict: åŒ…å«æ‰€æœ‰æŠ½å–æ•°æ®çš„å­—å…¸
    """
    if config is None:
        config = PipelineConfig()
    log.info("=" * 60)
    log.info(f"ğŸ“Š å¼€å§‹ä¸º {trade_date} æŠ½å–å…¨å¸‚åœºåŸå§‹æ•°æ®")
    log.info("=" * 60)

    dm = data.DataManager()
    stock_list = dm.get_stock_basic()
    ts_codes = stock_list["ts_code"].tolist()
    
    log.info(f"ğŸ¯ ç›®æ ‡è‚¡ç¥¨æ•°é‡: {len(ts_codes)} åª")

    # --- 1. æ‰¹é‡è·å–æˆªé¢æ•°æ® ---
    log.info("ğŸ“ˆ è·å–å½“æ—¥æˆªé¢æ•°æ®...")
    
    data_sources = [
        ("åŸºæœ¬æŒ‡æ ‡", lambda: dm.pro.daily_basic(trade_date=trade_date)),
        ("èµ„é‡‘æµå‘", lambda: dm.pro.moneyflow(trade_date=trade_date)),
        ("é¾™è™æ¦œ", lambda: dm.pro.top_list(trade_date=trade_date)),
        ("å¤§å®—äº¤æ˜“", lambda: dm.pro.block_trade(trade_date=trade_date))
    ]
    
    results = {}
    for name, func in data_sources:
        try:
            log.info(f"  ğŸ“Š è·å–{name}æ•°æ®...")
            result_data = func()
            results[name] = result_data
            count = len(result_data) if result_data is not None and not result_data.empty else 0
            log.info(f"  âœ… {name}: {count} æ¡è®°å½•")
        except Exception as e:
            log.warning(f"  âš ï¸  {name}è·å–å¤±è´¥: {e}")
            results[name] = pd.DataFrame()
    
    daily_basics_df = results.get("åŸºæœ¬æŒ‡æ ‡", pd.DataFrame())
    money_flow_df = results.get("èµ„é‡‘æµå‘", pd.DataFrame())
    top_list_df = results.get("é¾™è™æ¦œ", pd.DataFrame())
    block_trade_df = results.get("å¤§å®—äº¤æ˜“", pd.DataFrame())

    # --- 2. ã€ç¼“å­˜ä¼˜å…ˆã€‘è·å–æ—¶åºä»·æ ¼æ•°æ® ---
    log.info("  å¼€å§‹è·å–å„è‚¡ç¥¨çš„å†å²ä»·æ ¼ (ç¼“å­˜ä¼˜å…ˆ)...")
    start_date_lookback = (pd.to_datetime(trade_date) - timedelta(days=90)).strftime("%Y%m%d")
    prices_dict = {}

# æ­¥éª¤ A: æ£€æŸ¥æ•°æ®åº“ç¼“å­˜
    try:
        min_trading_days = 55
        # V3.1 ç»ˆæå¥å£®æ€§ä¿®å¤ï¼šåœ¨SQLæŸ¥è¯¢ä¸­æ˜ç¡®è¿›è¡Œæ—¥æœŸç±»å‹è½¬æ¢ï¼Œå½»åº•è§£å†³ç¼“å­˜æ£€æŸ¥å¤±æ•ˆé—®é¢˜
        query = text("""
            SELECT ts_code FROM ts_daily
            WHERE trade_date BETWEEN TO_DATE(:start_date, 'YYYYMMDD') AND TO_DATE(:end_date, 'YYYYMMDD') 
              AND ts_code = ANY(:ts_codes)
            GROUP BY ts_code 
            HAVING COUNT(trade_date) >= :min_days
        """)
        with dm.engine.connect() as conn:
            cached_stocks_result = conn.execute(query, {
                "start_date": start_date_lookback, "end_date": trade_date,
                "ts_codes": ts_codes, "min_days": PipelineConfig.MIN_TRADING_DAYS
            }).fetchall()
        cached_stocks = {row[0] for row in cached_stocks_result}
        log.info(f"    æ•°æ®åº“ç¼“å­˜æ£€æŸ¥å®Œæˆï¼š{len(cached_stocks)}/{len(ts_codes)} åªè‚¡ç¥¨å·²æœ‰å®Œæ•´æœ¬åœ°æ•°æ®ã€‚")
    except Exception as e:
        log.warning(f"    æ£€æŸ¥ç¼“å­˜å¤±è´¥: {e}ã€‚å°†å°è¯•å…¨é‡ä¸‹è½½ã€‚")
        cached_stocks = set()

    # æ­¥éª¤ B: ä»…ä¸‹è½½æ— ç¼“å­˜çš„æ•°æ® (å¸¦é‡è¯•æœºåˆ¶)
    stocks_to_download = sorted(list(set(ts_codes) - cached_stocks))
    if stocks_to_download:
        log.info(f"    éœ€è¦ä»ç½‘ç»œä¸‹è½½ {len(stocks_to_download)} åªè‚¡ç¥¨çš„æ•°æ®...")
        
        # --- ã€æ–°å¢ã€‘å®Œæ•´æ€§æ ¡éªŒä¸é‡è¯•é€»è¾‘ ---
        downloaded_data_raw = {}
        max_retries = PipelineConfig.MAX_RETRIES  # V2.9 æå‡å¥å£®æ€§ï¼šå¢åŠ ä¸Šå±‚é‡è¯•æ¬¡æ•°
        for attempt in range(max_retries + 1):
            # V2.9 ä¿®æ­£ï¼šæ¯æ¬¡é‡è¯•æ—¶ï¼Œéƒ½éœ€è¦é‡æ–°è®¡ç®—è¿˜éœ€è¦ä¸‹è½½çš„åˆ—è¡¨
            needed = [
                code
                for code in stocks_to_download
                if code not in downloaded_data_raw
                or downloaded_data_raw[code] is None
                or downloaded_data_raw[code].empty
            ]
            if not needed:
                break
            
            if attempt > 0:
                log.warning(f"    ç¬¬ {attempt} æ¬¡é‡è¯•ï¼Œä¸‹è½½å‰©ä½™çš„ {len(needed)} åªè‚¡ç¥¨...")

            chunk_size = PipelineConfig.DOWNLOAD_CHUNK_SIZE
            for i in range(0, len(needed), chunk_size):
                chunk = needed[i : i + chunk_size]
                log.info(f"      æ­£åœ¨ä¸‹è½½å— {i//chunk_size + 1}/{len(needed)//chunk_size + 1} (è‚¡ç¥¨ {i+1}-{i+len(chunk)})...")
                chunk_results = dm.run_batch_download(chunk, start_date_lookback, trade_date)
                downloaded_data_raw.update(chunk_results)
            
        # æ ¡éªŒæœ€ç»ˆç»“æœ
        missing_stocks = [code for code in stocks_to_download if code not in downloaded_data_raw or downloaded_data_raw[code] is None or downloaded_data_raw[code].empty]
        if missing_stocks:
            log.error(f"    è­¦å‘Šï¼šç»è¿‡ {max_retries} æ¬¡é‡è¯•åï¼Œä»æœ‰ {len(missing_stocks)} åªè‚¡ç¥¨æ•°æ®ä¸‹è½½å¤±è´¥ã€‚")
        # --- æ ¡éªŒé€»è¾‘ç»“æŸ ---

        for code, df in downloaded_data_raw.items():
            if df is not None and not df.empty:
                df['trade_date'] = pd.to_datetime(df['trade_date'])
                prices_dict[code] = df.set_index("trade_date")["close"]

    # æ­¥éª¤ C: ä»æ•°æ®åº“åŠ è½½å·²ç¼“å­˜çš„æ•°æ®
    if cached_stocks:
        log.info(f"    æ­£åœ¨ä»æ•°æ®åº“åˆ†æ‰¹åŠ è½½ {len(cached_stocks)} åªå·²ç¼“å­˜è‚¡ç¥¨çš„æ•°æ®...")
        # (æ­¤å¤„é€»è¾‘ä¸å˜)
        cached_list = list(cached_stocks)
        chunk_size = PipelineConfig.DB_CHUNK_SIZE
        for i in range(0, len(cached_list), chunk_size):
            chunk = cached_list[i:i+chunk_size]
            query = text("""
                SELECT ts_code, trade_date, close FROM ts_daily
                WHERE trade_date BETWEEN :start_date AND :end_date AND ts_code = ANY(:ts_codes)
            """)
            with dm.engine.connect() as conn:
                cached_df = pd.read_sql(query, conn, params={
                    "start_date": start_date_lookback, "end_date": trade_date, "ts_codes": chunk
                })
            
            if not cached_df.empty:
                cached_df['trade_date'] = pd.to_datetime(cached_df['trade_date'])
                for code, group in cached_df.groupby('ts_code'):
                    prices_dict[code] = group.set_index('trade_date')['close']

    # --- 3. åˆå¹¶ä¸æ ¼å¼åŒ– ---
    daily_prices_df = pd.DataFrame(prices_dict)

    log.info("ã€æ•°æ®æŠ½å–ã€‘æ‰€æœ‰åŸå§‹æ•°æ®æå–å®Œæ¯•ã€‚")

    return {
        "stock_list": stock_list,
        "ts_codes": ts_codes,
        "daily_prices_df": daily_prices_df,
        "daily_basics_df": daily_basics_df,
        "money_flow_df": money_flow_df,
        "top_list_df": top_list_df,
        "block_trade_df": block_trade_df,
    }


def calculate_and_save_factors(
    trade_date: str, 
    raw_data: Dict[str, Any], 
    all_fina_data: pd.DataFrame,
    config: Optional[PipelineConfig] = None
) -> bool:
    """
    æ­¥éª¤äºŒï¼šæ‰§è¡Œå› å­è®¡ç®—ä¸å­˜å‚¨ã€‚
    
    Args:
        trade_date: äº¤æ˜“æ—¥æœŸ YYYYMMDDæ ¼å¼
        raw_data: ä» extract_data å‡½æ•°è·å–çš„åŸå§‹æ•°æ®å­—å…¸
        all_fina_data: é¢„å…ˆè·å–çš„å…¨å¸‚åœºè´¢åŠ¡æ•°æ®
        config: ç®¡é“é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        
    Returns:
        bool: æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
        
    Raises:
        Exception: å½“æ•°æ®å†™å…¥æ•°æ®åº“å¤±è´¥æ—¶
    """
    if config is None:
        config = PipelineConfig()
    log.info("ã€å› å­è®¡ç®—ã€‘å¼€å§‹æ··åˆæ¨¡å¼è®¡ç®—æ‰€æœ‰å› å­...")
    dm = data.DataManager()
    ff = quant_engine.FactorFactory(_data_manager=dm)
    trade_date_dt = pd.to_datetime(trade_date)

    # ä»ä¼ å…¥çš„å­—å…¸ä¸­è§£åŒ…æ•°æ®
    ts_codes = raw_data["ts_codes"]
    daily_prices_df = raw_data["daily_prices_df"]
    daily_basics_df = raw_data["daily_basics_df"]
    money_flow_df = raw_data["money_flow_df"]

    results = {}

    # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šé«˜æ€§èƒ½çš„å‘é‡åŒ–è®¡ç®— ---
    log.info("  æ­£åœ¨å‘é‡åŒ–è®¡ç®—ï¼šä»·æ ¼ç±»ä¸æˆªé¢ç±»å› å­...")
    vectorized_factors = ["pe_ttm", "momentum", "volatility", "net_inflow_ratio"]
    financial_factors = ["roe", "growth_revenue_yoy", "debt_to_assets"] # å®šä¹‰è´¢åŠ¡å› å­

    if daily_prices_df is not None and not daily_prices_df.empty:
        prices = daily_prices_df.ffill()
        # åŠ¨é‡å› å­è®¡ç®— (21æ—¥åŠ¨é‡)
        if len(prices) >= config.MOMENTUM_LOOKBACK_DAYS:
            results["momentum"] = (
                prices.iloc[-1] / prices.iloc[-config.MOMENTUM_LOOKBACK_DAYS] - 1
            )
        
        # æ³¢åŠ¨ç‡å› å­è®¡ç®— (20æ—¥æ³¢åŠ¨ç‡ï¼Œå¹´åŒ–)
        if len(prices) >= config.VOLATILITY_LOOKBACK_DAYS:
            log_returns = np.log(prices / prices.shift(1))
            results["volatility"] = (
                log_returns.iloc[-config.VOLATILITY_LOOKBACK_DAYS:].std() * 
                np.sqrt(config.TRADING_DAYS_PER_YEAR)
            )

    if daily_basics_df is not None and not daily_basics_df.empty:
        basics = daily_basics_df.set_index("ts_code")
        results["pe_ttm"] = basics["pe_ttm"]
        if money_flow_df is not None and not money_flow_df.empty:
            flow = money_flow_df.drop(columns=["trade_date"]).set_index("ts_code")
            combined = basics.join(flow, how="inner")
            if not combined.empty and "amount" in combined.columns:
                amount_yuan = combined["amount"] * 1000
                net_inflow_yuan = (
                    combined["buy_lg_amount"] - combined["sell_lg_amount"]
                ) * 10000
                results["net_inflow_ratio"] = net_inflow_yuan.divide(
                    amount_yuan
                ).fillna(0)

    # --- ã€æ€§èƒ½ä¼˜åŒ–ã€‘ç¬¬äºŒéƒ¨åˆ†ï¼šåŸºäºé¢„å–æ•°æ®çš„è´¢åŠ¡å› å­è®¡ç®— ---
    log.info("  æ­£åœ¨åŸºäºé¢„å–æ•°æ®è®¡ç®—ï¼šè´¢åŠ¡ç±»å› å­...")
    if all_fina_data is not None and not all_fina_data.empty:
        # å¯¹é¢„å–çš„æ‰€æœ‰è´¢åŠ¡æ•°æ®è¿›è¡Œä¸€æ¬¡PITç­›é€‰
        try:
            # Use group_keys=False for pandas compatibility and better performance
            pit_fina_data = all_fina_data.groupby('ts_code', group_keys=False).apply(
                lambda x: dm.get_pit_financial_data(x, trade_date)
            ).reset_index(drop=True)
            
            if pit_fina_data.empty:
                log.warning("PITç­›é€‰åæ— æœ‰æ•ˆè´¢åŠ¡æ•°æ®")
            else:
                log.info(f"PITç­›é€‰å®Œæˆ: {len(pit_fina_data)} æ¡æœ‰æ•ˆè´¢åŠ¡è®°å½•")
                
        except Exception as e:
            log.error(f"PITç­›é€‰å¤±è´¥: {e}")
            pit_fina_data = pd.DataFrame()
        
        pit_fina_data = pit_fina_data.set_index('ts_code')
        results["roe"] = pit_fina_data["roe"]
        results["growth_revenue_yoy"] = pit_fina_data["or_yoy"] # Tushareä¸­å­—æ®µä¸º or_yoy
        results["debt_to_assets"] = pit_fina_data["debt_to_assets"]


    # --- ç¬¬ä¸‰éƒ¨åˆ†ï¼šå‰©ä½™å› å­ç»Ÿä¸€å¾ªç¯è®¡ç®— ---
    log.info("  æ­£åœ¨å¾ªç¯è®¡ç®—ï¼šç­¹ç ä¸ä»·å€¼ç±»å› å­...")
    params = {
        "date": trade_date,
        "start_date": (trade_date_dt - timedelta(days=90)).strftime("%Y%m%d"),
        "end_date": trade_date,
        "top_list_df": raw_data["top_list_df"],
        "block_trade_df": raw_data["block_trade_df"],
    }
    loop_factors = [f for f in FACTORS_TO_CALCULATE if f not in vectorized_factors and f not in financial_factors]

    for factor in loop_factors:
        log.info(f"    æ­£åœ¨è®¡ç®—å› å­: {factor}...")
        factor_series = pd.Series(index=ts_codes, dtype=float)
        for code in ts_codes:
            params["ts_code"] = code
            factor_series[code] = ff.calculate(factor, **params)
        results[factor] = factor_series

    # --- ç¬¬å››éƒ¨åˆ†ï¼šæ•´åˆä¸å­˜å‚¨ ---
    log.info("ã€æ•°æ®å…¥åº“ã€‘å¼€å§‹å­˜å‚¨å› å­æ•°æ®...")
    final_df = pd.DataFrame(results).reset_index().rename(columns={"index": "ts_code"})
    long_df = final_df.melt(
        id_vars="ts_code", var_name="factor_name", value_name="factor_value"
    ).dropna()
    long_df["trade_date"] = pd.to_datetime(trade_date)

    if long_df.empty:
        log.warning("æ²¡æœ‰æœ‰æ•ˆçš„å› å­æ•°æ®å¯ä»¥å­˜å…¥æ•°æ®åº“")
        return False

    try:
        with dm.engine.connect() as connection:
            with connection.begin():
                delete_sql = text(
                    "DELETE FROM factors_exposure WHERE trade_date = :trade_date"
                )
                connection.execute(delete_sql, {"trade_date": trade_date})
                long_df.to_sql(
                    "factors_exposure",
                    connection,
                    if_exists="append",
                    index=False,
                    chunksize=config.DB_WRITE_CHUNK_SIZE,
                )
        log.info(f"âœ… æˆåŠŸä¸º {trade_date} å†™å…¥ {len(long_df)} æ¡å› å­æ•°æ®")
        return True
        
    except Exception as e:
        log.critical(f"âŒ å› å­æ•°æ®å†™å…¥æ•°æ®åº“æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        return False


@retry_on_failure(max_retries=2, delay=1.0)
def get_latest_available_trade_date(dm: data.DataManager) -> str:
    """
    æ™ºèƒ½è·å–æœ€æ–°å¯ç”¨çš„äº¤æ˜“æ—¥æœŸ
    
    é€»è¾‘ï¼š
    1. å¦‚æœä»Šå¤©æ˜¯äº¤æ˜“æ—¥ä¸”å·²è¿‡æ”¶ç›˜æ—¶é—´(15:30)ï¼Œè¿”å›ä»Šå¤©
    2. å¦‚æœä»Šå¤©æ˜¯äº¤æ˜“æ—¥ä½†æœªåˆ°æ”¶ç›˜æ—¶é—´ï¼Œè¿”å›ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥
    3. å¦‚æœä»Šå¤©ä¸æ˜¯äº¤æ˜“æ—¥ï¼ˆå‘¨æœ«/èŠ‚å‡æ—¥ï¼‰ï¼Œè¿”å›æœ€è¿‘çš„äº¤æ˜“æ—¥
    
    Returns:
        str: æœ€æ–°å¯ç”¨äº¤æ˜“æ—¥æœŸ (YYYYMMDDæ ¼å¼)
    """
    now = datetime.now()
    current_date = now.strftime("%Y%m%d")
    
    # è·å–äº¤æ˜“æ—¥å†
    trade_dates = _get_trade_calendar(dm, now)
    if not trade_dates:
        log.warning("æ— æ³•è·å–äº¤æ˜“æ—¥å†ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ")
        return current_date
    
    # åˆ¤æ–­äº¤æ˜“æ—¥æœŸ
    if current_date in trade_dates:
        return _handle_trading_day(now, current_date, trade_dates)
    else:
        return _handle_non_trading_day(current_date, trade_dates)


def _get_trade_calendar(dm: data.DataManager, now: datetime) -> List[str]:
    """è·å–äº¤æ˜“æ—¥å†"""
    start_date = (now - timedelta(days=PipelineConfig.CALENDAR_BUFFER_DAYS)).strftime("%Y%m%d")
    end_date = (now + timedelta(days=PipelineConfig.CALENDAR_FUTURE_DAYS)).strftime("%Y%m%d")
    
    cal_df = dm.pro.trade_cal(exchange="", start_date=start_date, end_date=end_date)
    
    if cal_df is None or cal_df.empty:
        return []
    
    trade_dates = cal_df[cal_df["is_open"] == 1]["cal_date"].tolist()
    trade_dates.sort()
    return trade_dates


def _handle_trading_day(now: datetime, current_date: str, trade_dates: List[str]) -> str:
    """å¤„ç†å½“å¤©æ˜¯äº¤æ˜“æ—¥çš„æƒ…å†µ"""
    current_time = now.time()
    market_close_time = datetime.strptime(PipelineConfig.MARKET_CLOSE_TIME, "%H:%M").time()
    
    if current_time >= market_close_time:
        log.info(f"ä»Šå¤©({current_date})æ˜¯äº¤æ˜“æ—¥ä¸”å·²è¿‡æ”¶ç›˜æ—¶é—´ï¼Œä½¿ç”¨ä»Šå¤©ä½œä¸ºç›®æ ‡æ—¥æœŸ")
        return current_date
    else:
        # è·å–ä¸Šä¸€ä¸ªäº¤æ˜“æ—¥
        try:
            current_index = trade_dates.index(current_date)
            if current_index > 0:
                prev_trade_date = trade_dates[current_index - 1]
                log.info(f"ä»Šå¤©({current_date})æ˜¯äº¤æ˜“æ—¥ä½†æœªåˆ°æ”¶ç›˜æ—¶é—´ï¼Œä½¿ç”¨ä¸Šä¸€äº¤æ˜“æ—¥: {prev_trade_date}")
                return prev_trade_date
            else:
                log.info(f"ä»Šå¤©æ˜¯ç¬¬ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œä½¿ç”¨ä»Šå¤©: {current_date}")
                return current_date
        except ValueError:
            log.warning(f"åœ¨äº¤æ˜“æ—¥åˆ—è¡¨ä¸­æœªæ‰¾åˆ°ä»Šå¤©({current_date})ï¼Œä½¿ç”¨æœ€è¿‘äº¤æ˜“æ—¥")
            return max([d for d in trade_dates if d <= current_date], default=current_date)


def _handle_non_trading_day(current_date: str, trade_dates: List[str]) -> str:
    """å¤„ç†å½“å¤©ä¸æ˜¯äº¤æ˜“æ—¥çš„æƒ…å†µ"""
    recent_trade_dates = [d for d in trade_dates if d <= current_date]
    if recent_trade_dates:
        latest_trade_date = max(recent_trade_dates)
        log.info(f"ä»Šå¤©({current_date})ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œä½¿ç”¨æœ€è¿‘çš„äº¤æ˜“æ—¥: {latest_trade_date}")
        return latest_trade_date
    else:
        log.warning("æœªæ‰¾åˆ°åˆé€‚çš„äº¤æ˜“æ—¥ï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ")
        return current_date


def check_data_exists(dm: data.DataManager, trade_date: str) -> dict:
    """
    æ£€æŸ¥æŒ‡å®šäº¤æ˜“æ—¥çš„æ•°æ®æ˜¯å¦å·²å­˜åœ¨
    
    Args:
        dm: DataManagerå®ä¾‹
        trade_date: äº¤æ˜“æ—¥æœŸ (YYYYMMDDæ ¼å¼)
    
    Returns:
        dict: åŒ…å«å„ç±»æ•°æ®å­˜åœ¨çŠ¶æ€çš„å­—å…¸
    """
    result = {
        'trade_date': trade_date,
        'daily_data_exists': False,
        'factor_data_exists': False,
        'daily_data_count': 0,
        'factor_data_count': 0,
        'should_skip': False
    }
    
    try:
        with dm.engine.connect() as conn:
            # æ£€æŸ¥æ—¥çº¿æ•°æ®
            daily_count = conn.execute(text("""
                SELECT COUNT(*) FROM ts_daily 
                WHERE trade_date = :trade_date
            """), {"trade_date": trade_date}).scalar()
            
            result['daily_data_count'] = daily_count
            result['daily_data_exists'] = daily_count > 0
            
            # æ£€æŸ¥å› å­æ•°æ®
            factor_count = conn.execute(text("""
                SELECT COUNT(*) FROM factors_exposure 
                WHERE trade_date = :trade_date
            """), {"trade_date": trade_date}).scalar()
            
            result['factor_data_count'] = factor_count
            result['factor_data_exists'] = factor_count > 0
            
            # åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡
            # å¦‚æœå› å­æ•°æ®å·²å­˜åœ¨ä¸”æ•°é‡åˆç†ï¼ˆ>100ï¼‰ï¼Œåˆ™è·³è¿‡
            result['should_skip'] = result['factor_data_exists'] and result['factor_data_count'] > PipelineConfig.MIN_FACTOR_COUNT_THRESHOLD
            
    except Exception as e:
        log.error(f"æ£€æŸ¥æ•°æ®å­˜åœ¨æ€§å¤±è´¥: {e}")
    
    return result


def _determine_dates_to_process(dm: data.DataManager) -> list:
    """
    æ™ºèƒ½ç¡®å®šéœ€è¦å¤„ç†çš„äº¤æ˜“æ—¥åˆ—è¡¨
    
    Returns:
        list: éœ€è¦å¤„ç†çš„äº¤æ˜“æ—¥åˆ—è¡¨
    """
    log.info("===== æ™ºèƒ½ç¡®å®šéœ€è¦å¤„ç†çš„äº¤æ˜“æ—¥ =====")
    
    try:
        # é¦–å…ˆå°è¯•æ™ºèƒ½æ¨¡å¼ï¼šåªå¤„ç†æœ€æ–°çš„äº¤æ˜“æ—¥
        latest_trade_date = get_latest_available_trade_date(dm)
        log.info(f"ç¡®å®šæœ€æ–°å¯ç”¨äº¤æ˜“æ—¥: {latest_trade_date}")
        
        # æ£€æŸ¥è¯¥æ—¥æœŸçš„æ•°æ®æ˜¯å¦å·²å­˜åœ¨
        data_status = check_data_exists(dm, latest_trade_date)
        
        log.info(f"æ•°æ®å­˜åœ¨çŠ¶æ€æ£€æŸ¥:")
        log.info(f"  - æ—¥çº¿æ•°æ®: {'å­˜åœ¨' if data_status['daily_data_exists'] else 'ä¸å­˜åœ¨'} ({data_status['daily_data_count']} æ¡)")
        log.info(f"  - å› å­æ•°æ®: {'å­˜åœ¨' if data_status['factor_data_exists'] else 'ä¸å­˜åœ¨'} ({data_status['factor_data_count']} æ¡)")
        
        if data_status['should_skip']:
            log.info(f"âœ… {latest_trade_date} çš„æ•°æ®å·²å­˜åœ¨ä¸”å®Œæ•´ï¼Œè·³è¿‡å¤„ç†")
            return []
        
        # å¦‚æœéœ€è¦å¤„ç†ï¼Œåˆ™åªå¤„ç†è¿™ä¸€ä¸ªæ—¥æœŸ
        log.info(f"ğŸ“Š å°†å¤„ç†äº¤æ˜“æ—¥: {latest_trade_date}")
        return [latest_trade_date]
        
    except Exception as e:
        log.error(f"æ™ºèƒ½æ—¥æœŸç¡®å®šå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼: {e}")
        # å›é€€åˆ°ä¼ ç»Ÿçš„å†å²è¡¥æ¼æ¨¡å¼
        return get_missing_dates_traditional(dm)


def get_missing_dates_traditional(dm: data.DataManager) -> list:
    """
    ä¼ ç»Ÿçš„ç¼ºå¤±æ—¥æœŸè·å–æ–¹æ³•ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
    """
    log.info("===== ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ç¡®å®šéœ€è¦å¤„ç†çš„äº¤æ˜“æ—¥åˆ—è¡¨ =====")
    try:
        # è·å–æ•°æ®åº“ä¸­factors_exposureè¡¨çš„æœ€æ–°æ—¥æœŸ
        with dm.engine.connect() as connection:
            query_max_date = text("SELECT MAX(trade_date) FROM factors_exposure")
            last_processed_date_db = connection.execute(query_max_date).scalar_one_or_none()

        # ç¡®å®šå¼€å§‹è·å–äº¤æ˜“æ—¥å†çš„æ—¥æœŸ
        if last_processed_date_db:
            # ä»æ•°æ®åº“æœ€æ–°æ—¥æœŸçš„ä¸‹ä¸€å¤©å¼€å§‹æ£€æŸ¥
            start_cal_date = (pd.to_datetime(last_processed_date_db) + timedelta(days=1)).strftime("%Y%m%d")
        else:
            # å¦‚æœæ•°æ®åº“ä¸ºç©ºï¼Œåˆ™ä»ä¸€ä¸ªå¾ˆæ—©çš„æ—¥æœŸå¼€å§‹ï¼ˆä¾‹å¦‚2010å¹´ï¼‰
            start_cal_date = PipelineConfig.HISTORICAL_START_DATE
        
        end_cal_date = datetime.now().strftime("%Y%m%d")

        # è·å–Tushareäº¤æ˜“æ—¥å†
        cal_df = dm.pro.trade_cal(
            exchange="",
            start_date=start_cal_date,
            end_date=end_cal_date,
        )
        all_trade_dates_ts = set(cal_df[cal_df["is_open"] == 1]["cal_date"].tolist())

        # è·å–æ•°æ®åº“ä¸­å·²æœ‰çš„factors_exposureæ—¥æœŸ
        existing_factors_dates = set()
        if last_processed_date_db:
            with dm.engine.connect() as connection:
                query_existing_dates = text(
                    "SELECT DISTINCT trade_date FROM factors_exposure WHERE trade_date >= :start_date AND trade_date <= :end_date"
                )
                result = connection.execute(query_existing_dates, {
                    "start_date": start_cal_date, "end_date": end_cal_date
                }).fetchall()
                existing_factors_dates = {row[0] for row in result}

        # æ‰¾å‡ºæ‰€æœ‰ç¼ºå¤±çš„äº¤æ˜“æ—¥
        dates_to_process = sorted(list(all_trade_dates_ts - existing_factors_dates))

        if not dates_to_process:
            log.info("æ‰€æœ‰å†å²æ•°æ®å‡å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€è¡¥æ¼ã€‚ä»»åŠ¡ç»“æŸã€‚")
            return []
        else:
            log.info(f"å‘ç° {len(dates_to_process)} ä¸ªç¼ºå¤±æˆ–å¾…å¤„ç†çš„äº¤æ˜“æ—¥ã€‚")
            return dates_to_process

    except Exception as e:
        log.error(f"ä¼ ç»Ÿæ–¹æ³•ç¡®å®šäº¤æ˜“æ—¥åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
        return []


def run_daily_pipeline():
    """
    ã€V3.3 æ™ºèƒ½ä¼˜åŒ–ã€‘ç»Ÿä¸€æ•°æ®ç®¡é“ä¸»å‡½æ•°ã€‚
    æ–°å¢æ™ºèƒ½æ—¥æœŸåˆ¤æ–­ï¼Œé¿å…é‡å¤è·å–å·²å­˜åœ¨çš„æ•°æ®ã€‚
    """
    log.info("===== å¯åŠ¨æ™ºèƒ½ç»Ÿä¸€æ•°æ®ç®¡é“ =====")
    dm = data.DataManager()

    # ç¡®å®šéœ€è¦å¤„ç†çš„äº¤æ˜“æ—¥åˆ—è¡¨
    dates_to_process = _determine_dates_to_process(dm)
    if not dates_to_process:
        log.info("===== æ™ºèƒ½æ•°æ®ç®¡é“ä»»åŠ¡å®Œæˆï¼ˆæ— éœ€å¤„ç†ï¼‰=====")
        return

    # 2. å¾ªç¯å¤„ç†æ¯ä¸ªäº¤æ˜“æ—¥
    for current_trade_date in dates_to_process:
        log.info(f"===== å¼€å§‹æ‰§è¡Œ {current_trade_date} çš„ç»Ÿä¸€æ•°æ®ç®¡é“ä»»åŠ¡ =====")
        start_time = time.time()

        # --- å‰ç½®æ£€æŸ¥æœºåˆ¶ (é’ˆå¯¹å½“å‰æ—¥æœŸ) ---
        try:
            with dm.engine.connect() as connection:
                check_sql = text(
                    "SELECT 1 FROM factors_exposure WHERE trade_date = :trade_date LIMIT 1"
                )
                result = connection.execute(
                    check_sql, {"trade_date": current_trade_date}
                ).scalar_one_or_none()
            if result == 1:
                log.info(
                    f"æ£€æµ‹åˆ°æ•°æ®åº“ä¸­å·²å­˜åœ¨ {current_trade_date} çš„å› å­æ•°æ®ã€‚è·³è¿‡ã€‚"
                )
                continue # è·³è¿‡å½“å‰æ—¥æœŸï¼Œå¤„ç†ä¸‹ä¸€ä¸ª
        except Exception as e:
            log.error(f"åœ¨æ‰§è¡Œå‰ç½®æ£€æŸ¥æ—¶å‘ç”Ÿæ•°æ®åº“é”™è¯¯: {e}", exc_info=True)
            continue # å‘ç”Ÿé”™è¯¯åˆ™è·³è¿‡å½“å‰æ—¥æœŸ

        # --- æ‰§è¡Œæ•°æ®ç®¡é“ ---
        # æ­¥éª¤ä¸€ï¼šæŠ½å–æ•°æ®
        raw_data_dict = extract_data(current_trade_date)

        # ã€æ€§èƒ½ä¼˜åŒ–ã€‘æ­¥éª¤äºŒï¼šæ‰¹é‡é¢„å–å…¨å¸‚åœºè´¢åŠ¡æ•°æ®
        all_fina_data = _batch_fetch_financial_data(dm, raw_data_dict["ts_codes"])

        # æ­¥éª¤ä¸‰ï¼šè®¡ç®—å¹¶å­˜å‚¨å› å­
        calculate_and_save_factors(current_trade_date, raw_data_dict, all_fina_data)

        duration = time.time() - start_time
        log.info(
            f"===== {current_trade_date} çš„ç»Ÿä¸€æ•°æ®ç®¡é“ä»»åŠ¡å®Œæˆï¼æ€»è€—æ—¶: {duration:.2f} ç§’ã€‚====="
        )

    log.info("===== æ‰€æœ‰å¾…å¤„ç†äº¤æ˜“æ—¥çš„ç»Ÿä¸€æ•°æ®ç®¡é“ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼ =====")


if __name__ == "__main__":
    run_daily_pipeline()
    # input("\nä»»åŠ¡æ‰§è¡Œå®Œæ¯•ï¼ŒæŒ‰ Enter é”®é€€å‡º...") # ç§»é™¤inputï¼Œé¿å…éäº¤äº’å¼è¿è¡ŒæŒ‚èµ·
