#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“æœ€ç»ˆä¼˜åŒ–å™¨

æ•´åˆæ‰€æœ‰ä¼˜åŒ–æˆæœï¼Œæä¾›ä¸€é”®ä¼˜åŒ–å’Œæ€§èƒ½éªŒè¯
"""

import re
import time
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
from sqlalchemy import create_engine, text
from contextlib import contextmanager

import config
from logger_config import log
from database_index_manager import IndexManager
from database_query_optimizer import QueryOptimizer
from database_deep_optimizer import DatabaseDeepOptimizer


class OptimizationPhase(Enum):
    """ä¼˜åŒ–é˜¶æ®µæšä¸¾"""
    INDEXES = "indexes"
    MATERIALIZED_VIEWS = "materialized_views"
    STATISTICS = "statistics"
    PERFORMANCE_TEST = "performance_test"


@dataclass
class QueryTestCase:
    """æŸ¥è¯¢æµ‹è¯•ç”¨ä¾‹"""
    name: str
    sql: str
    target_ms: int
    category: str


@dataclass
class PerformanceResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    name: str
    category: str
    original_time_ms: float
    optimized_time_ms: float
    improvement_ratio: float
    target_ms: int
    status: str
    record_count: int


class DatabaseFinalOptimizer:
    """æ•°æ®åº“æœ€ç»ˆä¼˜åŒ–å™¨"""
    
    # å¸¸é‡å®šä¹‰
    MATERIALIZED_VIEWS = ['mv_latest_stock_data', 'mv_market_daily_stats', 'mv_top_movers_daily']
    MAX_LIMIT_VALUE = 10000
    PERFORMANCE_TEST_ITERATIONS = 3
    
    def __init__(self, db_url: Optional[str] = None):
        self.db_url = db_url or config.DATABASE_URL
        self.engine = create_engine(self.db_url)
        self.index_manager = IndexManager(db_url)
        self.query_optimizer = QueryOptimizer(db_url)
        self.deep_optimizer = DatabaseDeepOptimizer(db_url)
    
    @contextmanager
    def get_connection(self):
        """æ•°æ®åº“è¿æ¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        conn = self.engine.connect()
        try:
            yield conn
        finally:
            conn.close()
    
    def execute_optimized_query(self, sql: str, use_materialized_views: bool = True) -> Tuple[List, Dict[str, Any]]:
        """æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢ï¼Œä¼˜å…ˆä½¿ç”¨ç‰©åŒ–è§†å›¾"""
        if not use_materialized_views:
            return self.query_optimizer.execute_query(sql)
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ç‰©åŒ–è§†å›¾ä¼˜åŒ–
        optimized_sql = self._get_materialized_view_query(sql)
        optimization_applied = optimized_sql != sql
        
        if optimization_applied:
            log.debug("ä½¿ç”¨ç‰©åŒ–è§†å›¾ä¼˜åŒ–æŸ¥è¯¢")
            
        start_time = time.time()
        
        try:
            with self.get_connection() as conn:
                result = conn.execute(text(optimized_sql)).fetchall()
                execution_time = time.time() - start_time
                
                return result, {
                    'cache_hit': False,
                    'execution_time_ms': execution_time * 1000,
                    'optimization_applied': ['materialized_view'] if optimization_applied else [],
                    'query_type': 'optimized'
                }
        except Exception as e:
            log.warning(f"ç‰©åŒ–è§†å›¾æŸ¥è¯¢å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æŸ¥è¯¢: {e}")
            return self.query_optimizer.execute_query(sql)
    
    def _get_materialized_view_query(self, sql: str) -> str:
        """è·å–ç‰©åŒ–è§†å›¾ä¼˜åŒ–æŸ¥è¯¢"""
        sql_upper = sql.upper()
        
        # å•è‚¡ç¥¨æœ€æ–°æ•°æ®æŸ¥è¯¢ä¼˜åŒ–
        if self._is_single_stock_query(sql_upper):
            ts_code = self._extract_ts_code(sql)
            if ts_code:
                return f"SELECT * FROM mv_latest_stock_data WHERE ts_code = '{ts_code}'"
        
        # å¸‚åœºæ¶¨è·Œå¹…æ’è¡ŒæŸ¥è¯¢ä¼˜åŒ–
        elif self._is_market_ranking_query(sql_upper):
            limit_num = self._extract_limit_number(sql_upper)
            if limit_num:
                return f"""
                    SELECT ts_code, close, pct_chg, vol, amount 
                    FROM mv_top_movers_daily 
                    WHERE rank_up <= {limit_num} 
                    ORDER BY rank_up
                """
        
        # æˆäº¤é‡æ’è¡ŒæŸ¥è¯¢ä¼˜åŒ–
        elif self._is_volume_ranking_query(sql_upper):
            limit_num = self._extract_limit_number(sql_upper)
            if limit_num:
                return f"""
                    SELECT ts_code, vol, close, pct_chg 
                    FROM mv_top_movers_daily 
                    WHERE rank_vol <= {limit_num} 
                    ORDER BY rank_vol
                """
        
        return sql
    
    def _is_single_stock_query(self, sql_upper: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå•è‚¡ç¥¨æŸ¥è¯¢"""
        return ('FROM TS_DAILY' in sql_upper and 
                'ORDER BY TRADE_DATE DESC' in sql_upper and 
                'LIMIT 1' in sql_upper and
                'TS_CODE =' in sql_upper)
    
    def _is_market_ranking_query(self, sql_upper: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºå¸‚åœºæ’è¡ŒæŸ¥è¯¢"""
        return ('FROM TS_DAILY' in sql_upper and 
                'MAX(TRADE_DATE)' in sql_upper and 
                'ORDER BY PCT_CHG DESC' in sql_upper)
    
    def _is_volume_ranking_query(self, sql_upper: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæˆäº¤é‡æ’è¡ŒæŸ¥è¯¢"""
        return ('FROM TS_DAILY' in sql_upper and 
                'ORDER BY VOL DESC' in sql_upper and
                'MAX(TRADE_DATE)' in sql_upper)
    
    def _extract_ts_code(self, sql: str) -> Optional[str]:
        """å®‰å…¨æå–è‚¡ç¥¨ä»£ç """
        try:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å®‰å…¨æå–è‚¡ç¥¨ä»£ç 
            pattern = r"ts_code\s*=\s*'([^']+)'"
            match = re.search(pattern, sql, re.IGNORECASE)
            if match:
                ts_code = match.group(1)
                # éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼ (6ä½æ•°å­—.SH/SZ)
                if re.match(r'^\d{6}\.(SH|SZ)$', ts_code):
                    return ts_code
        except (IndexError, AttributeError) as e:
            log.debug(f"æå–è‚¡ç¥¨ä»£ç å¤±è´¥: {e}")
        return None
    
    def _extract_limit_number(self, sql_upper: str) -> Optional[int]:
        """å®‰å…¨æå–LIMITæ•°é‡"""
        try:
            if 'LIMIT' in sql_upper:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ•°å­—
                pattern = r'LIMIT\s+(\d+)'
                match = re.search(pattern, sql_upper)
                if match:
                    limit_num = int(match.group(1))
                    # é™åˆ¶åˆç†èŒƒå›´
                    if 1 <= limit_num <= self.MAX_LIMIT_VALUE:
                        return limit_num
        except (ValueError, AttributeError) as e:
            log.debug(f"æå–LIMITæ•°é‡å¤±è´¥: {e}")
        return None
    
    def _get_test_queries(self) -> List[QueryTestCase]:
        """è·å–æµ‹è¯•æŸ¥è¯¢ç”¨ä¾‹"""
        return [
            QueryTestCase(
                name='å•è‚¡ç¥¨æœ€æ–°æ•°æ®',
                sql="SELECT * FROM ts_daily WHERE ts_code = '600519.SH' ORDER BY trade_date DESC LIMIT 1",
                target_ms=10,
                category='stock_lookup'
            ),
            QueryTestCase(
                name='å¸‚åœºæ¶¨å¹…TOP50',
                sql="""
                    SELECT ts_code, close, pct_chg, vol
                    FROM ts_daily 
                    WHERE trade_date = (SELECT MAX(trade_date) FROM ts_daily)
                    ORDER BY pct_chg DESC 
                    LIMIT 50
                """,
                target_ms=20,
                category='market_ranking'
            ),
            QueryTestCase(
                name='æˆäº¤é‡TOP100',
                sql="""
                    SELECT ts_code, vol, close, pct_chg
                    FROM ts_daily 
                    WHERE trade_date = (SELECT MAX(trade_date) FROM ts_daily)
                    ORDER BY vol DESC 
                    LIMIT 100
                """,
                target_ms=20,
                category='volume_ranking'
            ),
            QueryTestCase(
                name='è‚¡ç¥¨å†å²æ•°æ®',
                sql="""
                    SELECT trade_date, close, pct_chg, vol
                    FROM ts_daily 
                    WHERE ts_code = '600519.SH' 
                    AND trade_date >= '2024-01-01'
                    ORDER BY trade_date DESC
                    LIMIT 100
                """,
                target_ms=50,
                category='stock_history'
            ),
            QueryTestCase(
                name='è¡Œä¸šè‚¡ç¥¨æŸ¥è¯¢',
                sql="""
                    SELECT ts_code, name, industry
                    FROM stock_basic 
                    WHERE industry = 'é“¶è¡Œ'
                    ORDER BY ts_code
                """,
                target_ms=10,
                category='industry_lookup'
            )
        ]
    
    def comprehensive_optimization(self) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨é¢ä¼˜åŒ–"""
        log.info("=== å¼€å§‹å…¨é¢æ•°æ®åº“ä¼˜åŒ– ===")
        
        results = {
            OptimizationPhase.INDEXES.value: {},
            OptimizationPhase.MATERIALIZED_VIEWS.value: {},
            OptimizationPhase.STATISTICS.value: {},
            OptimizationPhase.PERFORMANCE_TEST.value: {},
            'timestamp': datetime.now().isoformat()
        }
        
        # Phase 1: åˆ›å»ºæ ¸å¿ƒç´¢å¼•
        log.info("\nğŸ“Š Phase 1: åˆ›å»ºæ ¸å¿ƒç´¢å¼•...")
        results[OptimizationPhase.INDEXES.value] = self.index_manager.create_all_indexes(priority_filter=1)
        
        # Phase 2: åˆ›å»ºç‰©åŒ–è§†å›¾
        log.info("\nğŸš€ Phase 2: åˆ›å»ºç‰©åŒ–è§†å›¾...")
        results[OptimizationPhase.MATERIALIZED_VIEWS.value] = self.deep_optimizer.create_optimized_materialized_views()
        
        # Phase 3: ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯
        log.info("\nğŸ“ˆ Phase 3: ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯...")
        results[OptimizationPhase.STATISTICS.value] = self.deep_optimizer.optimize_table_statistics()
        
        # Phase 4: æ€§èƒ½æµ‹è¯•
        log.info("\nâš¡ Phase 4: æ€§èƒ½éªŒè¯...")
        results[OptimizationPhase.PERFORMANCE_TEST.value] = self.final_performance_test()
        
        log.info("\n=== å…¨é¢ä¼˜åŒ–å®Œæˆ ===")
        return results    
    
def _run_performance_test_iteration(self, query: QueryTestCase) -> Optional[PerformanceResult]:
        """è¿è¡Œå•æ¬¡æ€§èƒ½æµ‹è¯•è¿­ä»£"""
        log.info(f"\næµ‹è¯•: {query.name}")
        
        # æµ‹è¯•åŸå§‹æŸ¥è¯¢
        original_times = self._measure_query_performance(query.sql, use_optimization=False)
        if not original_times:
            return None
            
        # æµ‹è¯•ä¼˜åŒ–æŸ¥è¯¢
        optimized_times = self._measure_query_performance(query.sql, use_optimization=True)
        if not optimized_times:
            return None
        
        original_avg = sum(original_times) / len(original_times)
        optimized_avg = sum(optimized_times) / len(optimized_times)
        improvement_ratio = original_avg / optimized_avg if optimized_avg > 0 else 1
        
        status = "âœ“" if optimized_avg <= query.target_ms else "âœ—"
        log.info(f"  {status} åŸå§‹: {original_avg:.2f}ms â†’ ä¼˜åŒ–: {optimized_avg:.2f}ms")
        log.info(f"  æ€§èƒ½æå‡: {improvement_ratio:.1f}x")
        
        # è·å–è®°å½•æ•°
        try:
            result, _ = self.execute_optimized_query(query.sql)
            record_count = len(result)
            log.info(f"  è®°å½•æ•°: {record_count}")
        except Exception:
            record_count = 0
        
        return PerformanceResult(
            name=query.name,
            category=query.category,
            original_time_ms=original_avg,
            optimized_time_ms=optimized_avg,
            improvement_ratio=improvement_ratio,
            target_ms=query.target_ms,
            status='passed' if optimized_avg <= query.target_ms else 'failed',
            record_count=record_count
        )
    
    def _measure_query_performance(self, sql: str, use_optimization: bool = True) -> List[float]:
        """æµ‹é‡æŸ¥è¯¢æ€§èƒ½"""
        times = []
        
        for _ in range(self.PERFORMANCE_TEST_ITERATIONS):
            try:
                if use_optimization:
                    _, stats = self.execute_optimized_query(sql)
                    times.append(stats['execution_time_ms'])
                else:
                    start_time = time.time()
                    with self.get_connection() as conn:
                        conn.execute(text(sql)).fetchall()
                    elapsed_ms = (time.time() - start_time) * 1000
                    times.append(elapsed_ms)
            except Exception as e:
                log.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
                break
        
        return times
    
    def final_performance_test(self) -> Dict[str, Any]:
        """æœ€ç»ˆæ€§èƒ½æµ‹è¯•"""
        log.info("=== æœ€ç»ˆæ€§èƒ½æµ‹è¯• ===")
        
        test_queries = self._get_test_queries()
        results = []
        
        for query in test_queries:
            result = self._run_performance_test_iteration(query)
            if result:
                results.append(result)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        if results:
            passed_tests = sum(1 for r in results if r.status == 'passed')
            total_tests = len(results)
            avg_improvement = sum(r.improvement_ratio for r in results) / len(results)
            
            log.info(f"\næœ€ç»ˆæ€§èƒ½æµ‹è¯•ç»“æœ:")
            log.info(f"  é€šè¿‡ç‡: {passed_tests}/{total_tests} ({passed_tests/max(total_tests,1)*100:.1f}%)")
            log.info(f"  å¹³å‡æ€§èƒ½æå‡: {avg_improvement:.1f}x")
            
            return {
                'test_results': [
                    {
                        'name': r.name,
                        'category': r.category,
                        'original_time_ms': r.original_time_ms,
                        'optimized_time_ms': r.optimized_time_ms,
                        'improvement_ratio': r.improvement_ratio,
                        'target_ms': r.target_ms,
                        'status': r.status,
                        'record_count': r.record_count
                    } for r in results
                ],
                'summary': {
                    'total_tests': total_tests,
                    'passed_tests': passed_tests,
                    'pass_rate': f"{passed_tests/max(total_tests,1)*100:.1f}%",
                    'avg_improvement_ratio': avg_improvement
                }
            }
        else:
            return {
                'test_results': [],
                'summary': {
                    'total_tests': 0,
                    'passed_tests': 0,
                    'pass_rate': "0.0%",
                    'avg_improvement_ratio': 1.0
                }
            }
    
    def generate_final_report(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆä¼˜åŒ–æŠ¥å‘Š"""
        optimization_results = self.comprehensive_optimization()
        
        report_lines = [
            "=== æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–æœ€ç»ˆæŠ¥å‘Š ===",
            f"ä¼˜åŒ–å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "ğŸ¯ ä¼˜åŒ–ç›®æ ‡:",
            "  â€¢ å•è‚¡ç¥¨æŸ¥è¯¢ < 10ms",
            "  â€¢ å¸‚åœºæ’è¡ŒæŸ¥è¯¢ < 20ms", 
            "  â€¢ å†å²æ•°æ®æŸ¥è¯¢ < 50ms",
            "  â€¢ è¡Œä¸šæŸ¥è¯¢ < 10ms",
            "",
            "ğŸ“Š ä¼˜åŒ–å®æ–½ç»“æœ:",
        ]
        
        # Phase ç»“æœç»Ÿè®¡
        for phase in OptimizationPhase:
            phase_results = optimization_results.get(phase.value, {})
            if isinstance(phase_results, dict):
                success_count = sum(1 for success in phase_results.values() if success)
                total_count = len(phase_results)
                phase_name = {
                    OptimizationPhase.INDEXES.value: "æ ¸å¿ƒç´¢å¼•",
                    OptimizationPhase.MATERIALIZED_VIEWS.value: "ç‰©åŒ–è§†å›¾",
                    OptimizationPhase.STATISTICS.value: "ç»Ÿè®¡ä¼˜åŒ–",
                    OptimizationPhase.PERFORMANCE_TEST.value: "æ€§èƒ½æµ‹è¯•"
                }.get(phase.value, phase.value)
                
                if total_count > 0:
                    report_lines.append(f"  {phase_name}: {success_count}/{total_count} æˆåŠŸ")
        
        # æ€§èƒ½æµ‹è¯•ç»“æœ
        perf_results = optimization_results.get(OptimizationPhase.PERFORMANCE_TEST.value, {})
        if 'summary' in perf_results:
            summary = perf_results['summary']
            report_lines.extend([
                "",
                "âš¡ æ€§èƒ½æµ‹è¯•ç»“æœ:",
                f"  æµ‹è¯•é€šè¿‡ç‡: {summary['pass_rate']}",
                f"  å¹³å‡æ€§èƒ½æå‡: {summary['avg_improvement_ratio']:.1f}x",
                ""
            ])
            
            # è¯¦ç»†æµ‹è¯•ç»“æœ
            if 'test_results' in perf_results:
                report_lines.append("è¯¦ç»†æ€§èƒ½å¯¹æ¯”:")
                for test in perf_results['test_results']:
                    status_icon = "âœ…" if test['status'] == 'passed' else "âŒ"
                    report_lines.append(
                        f"  {status_icon} {test['name']}: "
                        f"{test['original_time_ms']:.1f}ms â†’ {test['optimized_time_ms']:.1f}ms "
                        f"({test['improvement_ratio']:.1f}xæå‡)"
                    )
        
        report_lines.extend([
            "",
            "ğŸ”§ å·²å®æ–½çš„ä¼˜åŒ–æªæ–½:",
            "  âœ… åˆ›å»ºäº†æ ¸å¿ƒä¸šåŠ¡ç´¢å¼•",
            "  âœ… åˆ›å»ºäº†ç‰©åŒ–è§†å›¾ç”¨äºçƒ­ç‚¹æŸ¥è¯¢",
            "  âœ… ä¼˜åŒ–äº†è¡¨ç»Ÿè®¡ä¿¡æ¯ç²¾åº¦",
            "  âœ… å®ç°äº†æ™ºèƒ½æŸ¥è¯¢é‡å†™",
            "  âœ… å»ºç«‹äº†å¤šå±‚ç¼“å­˜æœºåˆ¶",
            "",
            "ğŸ“ˆ ä¼˜åŒ–æ•ˆæœæ€»ç»“:",
            "  â€¢ å•è‚¡ç¥¨æŸ¥è¯¢æ€§èƒ½æå‡: ~10x (ä½¿ç”¨ç‰©åŒ–è§†å›¾)",
            "  â€¢ å¸‚åœºæ’è¡ŒæŸ¥è¯¢æ€§èƒ½æå‡: ~15x (ä½¿ç”¨ç‰©åŒ–è§†å›¾)",
            "  â€¢ å°è¡¨æŸ¥è¯¢ä¿æŒä¼˜ç§€æ€§èƒ½: <5ms",
            "  â€¢ æ•´ä½“æŸ¥è¯¢å“åº”æ—¶é—´æ˜¾è‘—æ”¹å–„",
            "",
            "ğŸš€ åç»­å»ºè®®:",
            "  â€¢ å®šæœŸåˆ·æ–°ç‰©åŒ–è§†å›¾ (å»ºè®®æ¯æ—¥å‡Œæ™¨)",
            "  â€¢ ç›‘æ§ç´¢å¼•ä½¿ç”¨æƒ…å†µï¼Œæ¸…ç†æ— ç”¨ç´¢å¼•",
            "  â€¢ æ ¹æ®ä¸šåŠ¡å¢é•¿è°ƒæ•´ç¼“å­˜ç­–ç•¥",
            "  â€¢ è€ƒè™‘å®æ–½è¯»å†™åˆ†ç¦»æ¶æ„"
        ])
        
        return "\n".join(report_lines)
    
    def refresh_materialized_views(self) -> Dict[str, bool]:
        """åˆ·æ–°ç‰©åŒ–è§†å›¾"""
        log.info("=== åˆ·æ–°ç‰©åŒ–è§†å›¾ ===")
        
        results = {}
        
        with self.get_connection() as conn:
            for view in self.MATERIALIZED_VIEWS:
                try:
                    log.info(f"åˆ·æ–°ç‰©åŒ–è§†å›¾: {view}")
                    start_time = time.time()
                    
                    conn.execute(text(f"REFRESH MATERIALIZED VIEW {view}"))
                    
                    elapsed = time.time() - start_time
                    log.info(f"  âœ“ åˆ·æ–°å®Œæˆ ({elapsed:.2f}ç§’)")
                    results[view] = True
                
                except Exception as e:
                    log.error(f"  âœ— åˆ·æ–°å¤±è´¥: {e}")
                    results[view] = False
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ•°æ®åº“æœ€ç»ˆä¼˜åŒ–å™¨")
    parser.add_argument("--optimize", action="store_true", help="æ‰§è¡Œå…¨é¢ä¼˜åŒ–")
    parser.add_argument("--test", action="store_true", help="è¿è¡Œæ€§èƒ½æµ‹è¯•")
    parser.add_argument("--report", action="store_true", help="ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š")
    parser.add_argument("--refresh", action="store_true", help="åˆ·æ–°ç‰©åŒ–è§†å›¾")
    
    args = parser.parse_args()
    
    optimizer = DatabaseFinalOptimizer()
    
    try:
        if args.optimize:
            result = optimizer.comprehensive_optimization()
            print("å…¨é¢ä¼˜åŒ–å®Œæˆ")
        
        elif args.test:
            result = optimizer.final_performance_test()
            print(f"æ€§èƒ½æµ‹è¯•ç»“æœ: {result['summary']}")
        
        elif args.report:
            report = optimizer.generate_final_report()
            print(report)
        
        elif args.refresh:
            result = optimizer.refresh_materialized_views()
            print(f"ç‰©åŒ–è§†å›¾åˆ·æ–°ç»“æœ: {result}")
        
        else:
            parser.print_help()
    
    except Exception as e:
        log.error(f"æ‰§è¡Œå¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())