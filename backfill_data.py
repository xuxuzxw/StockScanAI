# StockScanAI/backfill_data.py
import argparse
import time

import pandas as pd
from sqlalchemy import text

import config
import data
from logger_config import log
from progress_tracker import ProgressTracker, DataQualityChecker


def backfill_data_optimized(
    start_date: str,
    end_date: str,
    ts_codes: list = None,
    chunk_size: int = 200,
):
    """
    ã€V2.9 å¢å¼ºç‰ˆã€‘å…·å¤‡æ–­ç‚¹ç»­ä¼ ã€åœç‰Œå¡«å……å’Œè¯¦ç»†è¿›åº¦æ˜¾ç¤ºçš„é«˜æ€§èƒ½æ•°æ®å›å¡«å·¥å…·ã€‚
    - å¯åŠ¨æ—¶è‡ªåŠ¨æ£€æµ‹å·²å®Œæˆçš„è‚¡ç¥¨ï¼ˆåŒ…æ‹¬å·²å¡«å……åœç‰Œæ ‡è®°çš„ï¼‰ï¼Œå¹¶è·³è¿‡å®ƒä»¬ã€‚
    - æ–°å¢åœç‰Œæ—¥å¡«å……é€»è¾‘ï¼šå¯¹æˆåŠŸè¿”å›ä½†éƒ¨åˆ†æ—¥æœŸæ— æ•°æ®çš„è‚¡ç¥¨ï¼Œç”¨å ä½ç¬¦å¡«å……ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§ã€‚
    - åˆ©ç”¨å¼‚æ­¥å¹¶å‘ä¸‹è½½ï¼Œå¤§å¹…æå‡å…¨å¸‚åœºæ•°æ®å›å¡«é€Ÿåº¦ã€‚
    - å°†ä¸²è¡Œå¤„ç†æ”¹ä¸ºåˆ†å—æ‰¹é‡å¤„ç†ï¼Œå‡å°‘IOå¼€é”€ã€‚
    - å¢å¼ºè¿›åº¦æ˜¾ç¤ºå’Œé”™è¯¯å¤„ç†æœºåˆ¶ã€‚
    :param start_date: å¼€å§‹æ—¥æœŸ YYYYMMDD
    :param end_date: ç»“æŸæ—¥æœŸ YYYYMMDD
    :param ts_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†å…¨å¸‚åœº
    :param chunk_size: æ¯æ¬¡å¹¶å‘ä¸‹è½½çš„è‚¡ç¥¨æ•°é‡
    """
    log.info("=" * 60)
    log.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œé«˜æ€§èƒ½æ•°æ®å›å¡«ä»»åŠ¡ (v2.9)")
    log.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} â†’ {end_date}")
    log.info(f"âš™ï¸  å¹¶å‘è®¾ç½®: æ¯æ‰¹ {chunk_size} åªè‚¡ç¥¨")
    log.info("=" * 60)
    
    dm = data.DataManager(token=config.TUSHARE_TOKEN, db_url=config.DATABASE_URL)

    # --- V3.1 æœ€ç»ˆç‰ˆï¼šæ¢å¤å¹¶ä¿®å¤æ–­ç‚¹ç»­ä¼ æ£€æŸ¥é€»è¾‘ ---
    # 1. è·å–å…¨å¸‚åœºæˆ–æŒ‡å®šçš„è‚¡ç¥¨åˆ—è¡¨
    if ts_codes is None:
        log.info("ğŸ“Š è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨...")
        all_stocks = dm.get_stock_basic()
        initial_ts_codes = all_stocks["ts_code"].tolist()
        log.info(f"âœ“ åˆå§‹ç›®æ ‡å…± {len(initial_ts_codes)} åªè‚¡ç¥¨")
    else:
        initial_ts_codes = ts_codes
        log.info(f"ğŸ“Š ä½¿ç”¨æŒ‡å®šè‚¡ç¥¨åˆ—è¡¨ï¼Œå…± {len(initial_ts_codes)} åªè‚¡ç¥¨")

    # 2. è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ€»äº¤æ˜“æ—¥æ•°å’Œæ—¥æœŸåˆ—è¡¨
    log.info("ğŸ“… è·å–äº¤æ˜“æ—¥å†...")
    all_trade_dates = set()
    total_trade_days = 0
    try:
        trade_cal_df = dm.pro.trade_cal(start_date=start_date, end_date=end_date)
        open_trade_days_df = trade_cal_df[trade_cal_df["is_open"] == 1]
        total_trade_days = len(open_trade_days_df)
        all_trade_dates = set(open_trade_days_df["cal_date"])
        log.info(f"âœ“ æ—¶é—´èŒƒå›´å†…å…±æœ‰ {total_trade_days} ä¸ªäº¤æ˜“æ—¥")
    except Exception as e:
        log.error(f"âœ— è·å–äº¤æ˜“æ—¥å†å¤±è´¥: {e}")
        log.warning("âš  æ— æ³•æ‰§è¡Œæ–­ç‚¹ç»­ä¼ æ£€æŸ¥å’Œåœç‰Œå¡«å……ï¼Œå°†å°è¯•å…¨é‡å›å¡«")

    # 3. æŸ¥è¯¢æ•°æ®åº“ï¼Œæ‰¾å‡ºå·²å®Œæˆçš„è‚¡ç¥¨
    completed_codes = set()
    if total_trade_days > 0:
        log.info("ğŸ” æ£€æŸ¥æ•°æ®åº“ä¸­å·²å®Œæˆå›å¡«çš„è‚¡ç¥¨...")
        try:
            with dm.engine.connect() as conn:
                # ä¿®å¤ï¼šä½¿ç”¨ TO_DATE ç¡®ä¿æ—¥æœŸæ¯”è¾ƒçš„å¥å£®æ€§
                query = text(
                    """
                    SELECT ts_code FROM ts_daily
                    WHERE trade_date BETWEEN TO_DATE(:start, 'YYYYMMDD') AND TO_DATE(:end, 'YYYYMMDD')
                    GROUP BY ts_code
                    HAVING COUNT(1) >= :count
                """
                )
                result = conn.execute(
                    query,
                    {
                        "start": start_date,
                        "end": end_date,
                        "count": total_trade_days,
                    },
                )
                completed_codes = {row[0] for row in result}
                completion_rate = len(completed_codes) / len(initial_ts_codes) * 100
                log.info(f"âœ“ å‘ç° {len(completed_codes)} åªè‚¡ç¥¨æ•°æ®å·²å®Œæ•´ ({completion_rate:.1f}%)")
        except Exception as e:
            log.error(f"âœ— æ£€æŸ¥æ•°æ®åº“æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            log.warning("âš  å°†å°è¯•å…¨é‡å›å¡«")

    # 4. ç¡®å®šæœ€ç»ˆéœ€è¦å¤„ç†çš„è‚¡ç¥¨åˆ—è¡¨
    ts_codes_to_process = sorted(list(set(initial_ts_codes) - completed_codes))

    if not ts_codes_to_process:
        log.info("æ‰€æœ‰ç›®æ ‡è‚¡ç¥¨çš„æ•°æ®å‡å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€å›å¡«ã€‚ä»»åŠ¡ç»“æŸã€‚")
        return
    else:
        log.info(f"å°†å¤„ç† {len(ts_codes_to_process)} åªå°šæœªå®Œæˆæˆ–æ•°æ®ä¸å®Œæ•´çš„è‚¡ç¥¨ã€‚")
    # --- é¢„æ£€æŸ¥ç»“æŸ ---

    total_stocks = len(ts_codes_to_process)
    start_time_total = time.time()
    
    # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªå™¨
    progress_tracker = ProgressTracker(total_stocks, "æ•°æ®å›å¡«")
    log.info(f"ğŸ¯ å¼€å§‹å¤„ç† {total_stocks} åªè‚¡ç¥¨çš„æ•°æ®å›å¡«")

    max_chunk_retries = 3  # V3.0 ç»ˆæä¼˜åŒ–ï¼šä¸ºæ¯ä¸ªå—å¢åŠ é‡è¯•æœºåˆ¶
    processed_count = 0
    
    for i in range(0, total_stocks, chunk_size):
        chunk = ts_codes_to_process[i : i + chunk_size]

        # --- æ–°å¢ï¼šå—çº§é‡è¯•å¾ªç¯ ---
        chunk_success = False
        for attempt in range(max_chunk_retries + 1):
            chunk_start_time = time.time()
            chunk_num = i//chunk_size + 1
            total_chunks = (total_stocks + chunk_size - 1) // chunk_size
            
            log.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {chunk_num}/{total_chunks} ({len(chunk)} åªè‚¡ç¥¨)")
            if attempt > 0:
                log.warning(f"  ğŸ”„ é‡è¯• {attempt}/{max_chunk_retries}")

            try:
                # å¹¶å‘ä¸‹è½½æ—¥çº¿å’Œå¤æƒå› å­æ•°æ®
                log.info(f"  > æ­£åœ¨å¹¶å‘ä¸‹è½½ {len(chunk)} åªè‚¡ç¥¨çš„æ—¥çº¿æ•°æ®...")
                daily_data_dict = dm.run_batch_download(chunk, start_date, end_date)

                log.info(f"  > æ­£åœ¨å¢é‡ä¸‹è½½ {len(chunk)} åªè‚¡ç¥¨çš„å¤æƒå› å­...")
                adj_factor_dict = {}
                for code in chunk:
                    adj_df = dm.get_adj_factor(code, start_date, end_date)
                    if adj_df is not None and not adj_df.empty:
                        adj_factor_dict[code] = adj_df

                # æ•°æ®å¤„ç†ä¸å…¥åº“
                log.info(f"  > æ­£åœ¨å¤„ç†æ•°æ®å¹¶æ‰¹é‡å†™å…¥æ•°æ®åº“...")
                processed_daily_dfs = []
                adj_dfs = []
                for code in chunk:
                    daily_df = daily_data_dict.get(code)
                    # V3.0 å¥å£®æ€§ä¿®å¤ï¼šåªæœ‰åœ¨ä¸‹è½½æˆåŠŸæ—¶æ‰è·å–å¤æƒå› å­å’Œå¡«å……åœç‰Œ
                    if daily_df is None:
                        log.warning(f"  > {code} æ—¥çº¿æ•°æ®ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡åç»­å¤„ç†ã€‚")
                        continue
                    
                    adj_df = adj_factor_dict.get(code)

                    # --- åœç‰Œæ•°æ®å¡«å……é€»è¾‘ ---
                    if all_trade_dates:
                        existing_dates = set()
                        if not daily_df.empty:
                            existing_dates = set(pd.to_datetime(daily_df["trade_date"]).dt.strftime('%Y%m%d'))

                        missing_dates = all_trade_dates - existing_dates

                        if missing_dates:
                            log.debug(f"  > å¡«å…… {code} çš„ {len(missing_dates)} ä¸ªç¼ºå¤±/åœç‰Œæ—¥...")
                            placeholder_df = pd.DataFrame({ "ts_code": code, "trade_date": list(missing_dates), "vol": 0, "amount": 0 })
                            daily_df = pd.concat([daily_df, placeholder_df], ignore_index=True)

                    if not daily_df.empty:
                        processed_daily_dfs.append(daily_df)
                    if adj_df is not None and not adj_df.empty:
                        adj_dfs.append(adj_df)

                if processed_daily_dfs:
                    dm._upsert_data(
                        "ts_daily",
                        pd.concat(processed_daily_dfs, ignore_index=True),
                        ["trade_date", "ts_code"],
                    )
                if adj_dfs:
                    dm._upsert_data(
                        "ts_adj_factor",
                        pd.concat(adj_dfs, ignore_index=True),
                        ["trade_date", "ts_code"],
                    )

                chunk_duration = time.time() - chunk_start_time
                processed_count += len(chunk)
                
                # æ›´æ–°è¿›åº¦
                progress_tracker.update(len(chunk), f"æ‰¹æ¬¡{chunk_num}")
                
                log.info(f"  âœ… æ‰¹æ¬¡å¤„ç†æˆåŠŸï¼Œè€—æ—¶: {chunk_duration:.2f}ç§’ï¼Œå¹³å‡: {chunk_duration/len(chunk):.2f}ç§’/è‚¡")
                chunk_success = True
                break  # æˆåŠŸå¤„ç†ï¼Œè·³å‡ºé‡è¯•å¾ªç¯

            except Exception as e:
                log.error(f"âŒ æ‰¹æ¬¡ {chunk_num} (ç¬¬ {attempt+1} æ¬¡å°è¯•) å¤„ç†å¤±è´¥: {e}")
                if attempt == max_chunk_retries:
                    log.critical(f"  ğŸ’¥ æ‰¹æ¬¡ {chunk_num} åœ¨å°è¯• {max_chunk_retries+1} æ¬¡åå½»åº•å¤±è´¥ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    progress_tracker.mark_failed(len(chunk), f"æ‰¹æ¬¡{chunk_num}å½»åº•å¤±è´¥")
                else:
                    sleep_time = 2 ** (attempt + 1)  # æŒ‡æ•°é€€é¿
                    log.info(f"  â³ ç­‰å¾… {sleep_time} ç§’åé‡è¯•...")
                    time.sleep(sleep_time)
        
        if not chunk_success:
            log.error(f"âš ï¸  æ‰¹æ¬¡ {chunk_num} æœ€ç»ˆå¤±è´¥ï¼Œå·²è·³è¿‡")

    # å®Œæˆè¿›åº¦è·Ÿè¸ª
    progress_tracker.finish()
    
    # æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
    log.info("ğŸ” å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥...")
    quality_checker = DataQualityChecker(dm)
    quality_results = quality_checker.batch_quality_check(
        ts_codes_to_process[:100],  # æŠ½æ ·æ£€æŸ¥å‰100åªè‚¡ç¥¨
        start_date, 
        end_date
    )
    
    total_duration = time.time() - start_time_total
    log.info("=" * 60)
    log.info(f"ğŸ‰ å…¨éƒ¨æ•°æ®å›å¡«ä»»åŠ¡å®Œæˆï¼")
    log.info(f"â±ï¸  æ€»è€—æ—¶: {total_duration/60:.2f} åˆ†é’Ÿ")
    log.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {processed_count}/{total_stocks} åªè‚¡ç¥¨")
    log.info("=" * 60)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Aè‚¡å†å²æ•°æ®å›å¡«å·¥å…· (é«˜æ€§èƒ½-æ–­ç‚¹ç»­ä¼ ç‰ˆ)")
    parser.add_argument("--start", required=True, help="å¼€å§‹æ—¥æœŸ YYYYMMDD")
    parser.add_argument("--end", required=True, help="ç»“æŸæ—¥æœŸ YYYYMMDD")
    parser.add_argument(
        "--stocks", nargs="*", default=None, help="è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼Œä¸å¡«åˆ™ä¸ºå…¨å¸‚åœº"
    )
    parser.add_argument(
        "--chunk", type=int, default=200, help="æ¯æ¬¡å¹¶å‘è¯·æ±‚çš„è‚¡ç¥¨æ•°é‡"
    )

    args = parser.parse_args()

    backfill_data_optimized(args.start, args.end, args.stocks, args.chunk)